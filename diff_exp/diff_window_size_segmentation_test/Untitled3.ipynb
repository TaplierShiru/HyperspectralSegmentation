{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9172d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/makitorch')\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/')\n",
    "\n",
    "PREFIX_INFO_PATH = '/home/rustam/hyperspecter_segmentation/danil_cave/kfolds_data/kfold0'\n",
    "PATH_DATA = '/raid/rustam/hyperspectral_dataset/new_cropped_hsi_data'\n",
    "\n",
    "\n",
    "from multiprocessing.dummy import Pool\n",
    "from multiprocessing import shared_memory\n",
    "\n",
    "from makitorch import *\n",
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import comet_ml\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "from Losses import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from makitorch.architectures.U2Net import U2Net\n",
    "\n",
    "from hsi_dataset_api import HsiDataset\n",
    "\n",
    "from makitorch.dataloaders.HsiDataloader import HsiDataloader\n",
    "from makitorch.architectures.Unet import Unet, UnetWithFeatureSelection\n",
    "from makitorch.loss import muti_bce_loss_fusion\n",
    "from sklearn.metrics import jaccard_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "from makitorch.data_tools.augmentation import DataAugmentator\n",
    "from makitorch.data_tools.augmentation import BaseDataAugmentor\n",
    "from makitorch.data_tools.preprocessing import BaseDataPreprocessor\n",
    "from makitorch.data_tools.preprocessing import DataPreprocessor\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from hsi_dataset_api import HsiDataset\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def cut_into_parts(\n",
    "        image: np.ndarray, mask: np.ndarray, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    image_parts_list = []\n",
    "    mask_parts_list = []\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            img_part = image[:, \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "            mask_part = mask[\n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "\n",
    "            image_parts_list.append(img_part)\n",
    "            mask_parts_list.append(mask_part)\n",
    "    return image_parts_list, mask_parts_list\n",
    "\n",
    "\n",
    "class ShmData:\n",
    "\n",
    "    def __init__(self, shm_name, shape, dtype):\n",
    "        self.shm_name = shm_name\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "class DatasetCreator:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_path: str,\n",
    "            preprocessing: Optional[Union[DataPreprocessor, Callable]] = BaseDataPreprocessor(),\n",
    "            indices = None,\n",
    "            cut_window=(8, 8),\n",
    "            map_mask_to_class=False,\n",
    "            create_shared_memory=False):\n",
    "        self.dataset = HsiDataset(data_path)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.cut_window = cut_window\n",
    "        self.map_mask_to_class = map_mask_to_class\n",
    "        self.create_shared_memory = create_shared_memory\n",
    "        \n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        \n",
    "        for idx, data_point in tqdm(enumerate(self.dataset.data_iterator(opened=True, shuffle=False))):\n",
    "            if indices is not None and idx not in indices:\n",
    "                continue\n",
    "            image, mask = data_point.hsi, data_point.mask\n",
    "            if cut_window is not None:\n",
    "                image_parts, mask_parts = self._cut_with_window(image, mask, cut_window)\n",
    "                self.images += image_parts\n",
    "                self.masks += mask_parts\n",
    "            else:\n",
    "                self.images.append(image)\n",
    "                self.masks.append(mask)\n",
    "        print(\"Preprocess data...\")\n",
    "        self.images = self.images[:100]\n",
    "        self.masks = self.masks[:100]\n",
    "        if self.preprocessing is not None:\n",
    "            self.images, self.masks = self.preprocessing(\n",
    "                self.images, self.masks, map_mask_to_class=map_mask_to_class\n",
    "            )\n",
    "\n",
    "        # Create shared memory\n",
    "        if create_shared_memory:\n",
    "            print('Create shared memory...')\n",
    "            # First - map images and masks into np\n",
    "            self.images = np.asarray(self.images, dtype=np.float32)\n",
    "            self.masks = np.asarray(self.masks, dtype=np.int64)\n",
    "            # Imgs\n",
    "            shm_imgs = shared_memory.SharedMemory(create=True, size=self.images.nbytes)\n",
    "            shm_imgs_arr = np.ndarray(self.images.shape, dtype=self.images.dtype, buffer=shm_imgs.buf)\n",
    "            shm_imgs_arr[:] = self.images[:]\n",
    "            # Masks\n",
    "            shm_masks = shared_memory.SharedMemory(create=True, size=self.masks.nbytes)\n",
    "            shm_masks_arr = np.ndarray(self.masks.shape, dtype=self.masks.dtype, buffer=shm_masks.buf)\n",
    "            shm_masks_arr[:] = self.masks[:]\n",
    "            print(\"Shared memory are created for imgs and masks!\")\n",
    "            self.data_shm_imgs = ShmData(\n",
    "                shm_name=shm_imgs.name, shape=self.images.shape, \n",
    "                dtype=self.images.dtype\n",
    "            )\n",
    "            self.data_shm_masks = ShmData(\n",
    "                shm_name=shm_masks.name, shape=self.masks.shape,\n",
    "                dtype=self.masks.dtype\n",
    "            )\n",
    "                    \n",
    "    \n",
    "    def _cut_with_window(self, image, mask, cut_window):\n",
    "        assert len(cut_window) == 2\n",
    "        h_win, w_win = cut_window\n",
    "        _, h, w = image.shape\n",
    "        h_parts = h // h_win\n",
    "        w_parts = w // w_win\n",
    "        if h % h_win != 0:\n",
    "            print(f\"{h % h_win} pixels will be dropped by h axis. Input shape={image.shape}\")\n",
    "\n",
    "        if w % w_win != 0:\n",
    "            print(f\"{w % w_win} pixels will be dropped by w axis. Input shape={image.shape}\")\n",
    "        return cut_into_parts(\n",
    "            image=image, mask=mask, h_parts=h_parts, w_parts=w_parts,\n",
    "            h_win=h_win, w_win=w_win\n",
    "        )\n",
    "\n",
    "\n",
    "# Define a `worker_init_fn` that configures each dataset copy differently\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset = worker_info.dataset\n",
    "    shared_memory_imgs_data: ShmData = dataset.shared_memory_imgs_data\n",
    "    shared_memory_masks_data: ShmData = dataset.shared_memory_masks_data\n",
    "    if shared_memory_imgs_data is not None and shared_memory_masks_data is not None:\n",
    "        # Take array from memory\n",
    "        existing_shm_imgs = shared_memory.SharedMemory(name=shared_memory_imgs_data.shm_name)\n",
    "        dataset_imgs_np = np.ndarray(\n",
    "            shared_memory_imgs_data.shape, \n",
    "            dtype=shared_memory_imgs_data.dtype, buffer=existing_shm_imgs.buf\n",
    "        )\n",
    "        dataset.shm_imgs = existing_shm_imgs\n",
    "        existing_shm_masks = shared_memory.SharedMemory(name=shared_memory_masks_data.shm_name)\n",
    "        dataset_masks_np = np.ndarray(\n",
    "            shared_memory_masks_data.shape, \n",
    "            dtype=shared_memory_masks_data.dtype, buffer=existing_shm_masks.buf\n",
    "        )\n",
    "        dataset.shm_masks = existing_shm_masks\n",
    "    else:\n",
    "        assert dataset.images is not None and dataset.masks is not None\n",
    "        dataset_imgs_np = dataset.images\n",
    "        dataset_masks_np = dataset.masks\n",
    "    overall_start = 0\n",
    "    overall_end = len(dataset_imgs_np)\n",
    "    # configure the dataset to only process the split workload\n",
    "    per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "    worker_id = worker_info.id\n",
    "    print(f\"id={worker_id}, num_workers={worker_info.num_workers}\")\n",
    "    start = overall_start + worker_id * per_worker\n",
    "    end = min(start + per_worker, overall_end)\n",
    "    dataset.images = list(dataset_imgs_np[start:end])\n",
    "    dataset.masks = list(dataset_masks_np[start:end])\n",
    "\n",
    "\n",
    "class HsiDataloaderCutter(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images, masks,\n",
    "            preprocessing: Optional[Union[DataPreprocessor, Callable]] = BaseDataPreprocessor(),\n",
    "            augmentation: Optional[Union[DataAugmentator, Callable]] = BaseDataAugmentor(),\n",
    "            indices = None,\n",
    "            shuffle_data=False,\n",
    "            cut_window=(8, 8),\n",
    "            map_mask_to_class=False,\n",
    "            data_start=None, data_end=None,\n",
    "            shared_memory_imgs_data: ShmData = None,\n",
    "            shared_memory_masks_data: ShmData = None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.shuffle_data = shuffle_data\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.cut_window = cut_window\n",
    "        self.map_mask_to_class = map_mask_to_class\n",
    "        self.shared_memory_imgs_data = shared_memory_imgs_data\n",
    "        self.shared_memory_masks_data = shared_memory_masks_data\n",
    "        \n",
    "        self.shm_imgs = None\n",
    "        self.shm_masks = None\n",
    "\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert self.images is not None and self.masks is not None\n",
    "        if self.shuffle_data:\n",
    "            self.images, self.masks = shuffle(self.images, self.masks)\n",
    "        \n",
    "        for image, mask in zip(self.images, self.masks):\n",
    "            yield self.augmentation(\n",
    "                image, mask, \n",
    "                map_mask_to_class=self.map_mask_to_class\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b632f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda:0'\n",
    "pca_explained_variance = np.load(f'{PREFIX_INFO_PATH}/kfold0_PcaExplainedVariance_.npy')\n",
    "pca_mean = np.load(f'{PREFIX_INFO_PATH}/kfold0_PcaMean.npy')\n",
    "pca_components = np.load(f'{PREFIX_INFO_PATH}/kfold0_PcaComponents.npy')\n",
    "\n",
    "\n",
    "def pca_transformation(x):\n",
    "    if len(x.shape) == 3:\n",
    "        x_t = x.reshape((x.shape[0], -1)) # (C, H, W) -> (C, H * W)\n",
    "        x_t = np.transpose(x_t, (1, 0)) # (C, H * W) -> (H * W, C)\n",
    "        x_t = x_t - pca_mean\n",
    "        x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "        return x_t.reshape((x.shape[1], x.shape[2], pca_components.shape[0])).astype(np.float32, copy=False) # (H, W, N)\n",
    "    elif len(x.shape) == 4:\n",
    "        # x - (N, C, H, W)\n",
    "        x_t = np.transpose(x, (0, 2, 3, 1)) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x_t = x_t - pca_mean\n",
    "        x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "        x_t = np.transpose(x_t, (0, -1, 1, 2)) # (N, H, W, C) -> (N, C, H, W)\n",
    "        return x_t.astype(np.float32, copy=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown shape={x.shape}, must be of len 3 or 4.\")\n",
    "\n",
    "def standartization(img, mean, std):\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    return img\n",
    "\n",
    "def standartization_pool(mean, std):\n",
    "    # X shape - (N, C, H, W)\n",
    "    # from shape (comp,) -> (1, comp, 1, 1)\n",
    "    mean = np.expand_dims(np.expand_dims(np.array(mean, dtype=np.float32), axis=-1), axis=-1)\n",
    "    std = np.expand_dims(np.expand_dims(np.array(std, dtype=np.float32), axis=-1), axis=-1)\n",
    "    \n",
    "    return lambda x: standartization(x, mean=mean, std=std)\n",
    "\n",
    "\n",
    "def mask2class(mask):\n",
    "    # Calculate which class have more pixel count\n",
    "    max_value = -1\n",
    "    pixel_count = -1\n",
    "    for class_indx in np.unique(mask):\n",
    "        pix_count_s = np.sum(mask == class_indx)\n",
    "        if pix_count_s > pixel_count:\n",
    "            max_value = class_indx\n",
    "            pixel_count = pix_count_s\n",
    "    assert max_value != -1\n",
    "    return np.array([max_value], dtype=np.int64) \n",
    "\n",
    "\n",
    "def preprocessing(imgs, masks, map_mask_to_class=False, split_size=256):\n",
    "    with open(f'{PREFIX_INFO_PATH}/data_standartization_params_kfold0.json', 'r') as f:\n",
    "        data_standartization_params = json.load(f)\n",
    "    mean = data_standartization_params.get('means')\n",
    "    std = data_standartization_params.get('stds')\n",
    "    assert mean is not None and std is not None\n",
    "    print('Create np array of imgs and masks...')\n",
    "    imgs_np = np.asarray(imgs, dtype=np.float32) # (N, 237, 1, 1)\n",
    "    masks_np = np.asarray(masks, dtype=np.int64) # (N, 1, 1, 3)\n",
    "    print(\"Split imgs dataset...\")\n",
    "    imgs_split_np = np.array_split(imgs_np, split_size) # (split_size, Ns, 237, 1, 1)\n",
    "    print('Start preprocess images...')\n",
    "    # Wo PCA\n",
    "    # _images = [np.transpose(image, (1, 2, 0)) for image in imgs]\n",
    "    # W Pca\n",
    "    with Pool(18) as p:\n",
    "        _images = list(tqdm(p.imap(\n",
    "                pca_transformation, \n",
    "                imgs_split_np,\n",
    "                #chunksize=1\n",
    "            ), total=len(imgs_split_np))\n",
    "        )\n",
    "        _images = list(tqdm(p.imap(\n",
    "            standartization_pool(mean=mean, std=std), \n",
    "            _images,\n",
    "            #chunksize=1\n",
    "            ), total=len(imgs_split_np))\n",
    "        )\n",
    "    _images = list(np.concatenate(_images, axis=0)) # (split_size, Ns, 237, 1, 1) -> (split_size * Ns, 237, 1, 1)\n",
    "    print(\"Preprocess masks...\")\n",
    "    _masks = list(np.transpose(masks_np[..., 0:1], (0, -1, 1, 2)))\n",
    "    print(\"Finish preprocess!\")\n",
    "    if map_mask_to_class:\n",
    "        _masks = [mask2class(mask) for mask in _masks]\n",
    "    return _images, _masks\n",
    "\n",
    "\n",
    "def test_augmentation(image, mask, **kwargs):\n",
    "    image = torch.from_numpy(image)\n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    mask = torch.from_numpy(mask)\n",
    "    \n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def augmentation(image, mask, map_mask_to_class=False):\n",
    "    image = torch.from_numpy(image)\n",
    "    mask = torch.from_numpy(mask)\n",
    "    angle = T.RandomRotation.get_params((-30, 30))\n",
    "    image = TF.rotate(image, angle, interpolation=T.InterpolationMode.BILINEAR)\n",
    "    if not map_mask_to_class:\n",
    "        mask = TF.rotate(mask, angle, interpolation=T.InterpolationMode.NEAREST)\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "        if not map_mask_to_class:\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "        if not map_mask_to_class:\n",
    "            mask = TF.vflip(mask)\n",
    "    \n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b65c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "361it [00:13, 26.99it/s]\n",
      "100%|██████████| 256/256 [00:00<00:00, 23433.91it/s]\n",
      "100%|██████████| 256/256 [00:00<00:00, 37875.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess data...\n",
      "Create np array of imgs and masks...\n",
      "Split imgs dataset...\n",
      "Start preprocess images...\n",
      "Preprocess masks...\n",
      "Finish preprocess!\n",
      "Create shared memory...\n",
      "Shared memory are created for imgs and masks!\n"
     ]
    }
   ],
   "source": [
    "cut_window=(1,1)\n",
    "train_indices = np.load(f'{PREFIX_INFO_PATH}/kfold0_indx_train.npy')[:1]\n",
    "dataset_creator_train = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=train_indices, cut_window=cut_window,\n",
    "    create_shared_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40f49567",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = HsiDataloaderCutter(\n",
    "    images=None, masks=None,\n",
    "    preprocessing=preprocessing, \n",
    "    augmentation=augmentation, indices=train_indices,\n",
    "    shuffle_data=True, cut_window=cut_window,\n",
    "    shared_memory_imgs_data=dataset_creator_train.data_shm_imgs,\n",
    "    shared_memory_masks_data=dataset_creator_train.data_shm_masks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d43a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=5, \n",
    "    num_workers=4, pin_memory=False, prefetch_factor=2,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3c1d76ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=0, num_workers=4\n",
      "id=1, num_workers=4id=2, num_workers=4\n",
      "\n",
      "id=3, num_workers=4\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "dict_used = dict([(str(i), False) for i in range(100)])\n",
    "for in_x, target in iter(train_loader):\n",
    "    for target_s in target:\n",
    "        dict_used[str(int(target_s[0][0]))] = True\n",
    "    if counter % 1_000 == 0:\n",
    "        print(counter)\n",
    "    counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7d7bff7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e534c1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 17, 1, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c19815e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 1]), tensor(78))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape, target[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "edf1ea6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': False,\n",
       " '1': True,\n",
       " '2': True,\n",
       " '3': True,\n",
       " '4': True,\n",
       " '5': True,\n",
       " '6': True,\n",
       " '7': True,\n",
       " '8': True,\n",
       " '9': True,\n",
       " '10': True,\n",
       " '11': True,\n",
       " '12': True,\n",
       " '13': True,\n",
       " '14': True,\n",
       " '15': True,\n",
       " '16': True,\n",
       " '17': True,\n",
       " '18': True,\n",
       " '19': True,\n",
       " '20': True,\n",
       " '21': True,\n",
       " '22': True,\n",
       " '23': True,\n",
       " '24': True,\n",
       " '25': True,\n",
       " '26': True,\n",
       " '27': True,\n",
       " '28': True,\n",
       " '29': True,\n",
       " '30': True,\n",
       " '31': True,\n",
       " '32': True,\n",
       " '33': True,\n",
       " '34': True,\n",
       " '35': True,\n",
       " '36': True,\n",
       " '37': True,\n",
       " '38': True,\n",
       " '39': True,\n",
       " '40': True,\n",
       " '41': True,\n",
       " '42': True,\n",
       " '43': True,\n",
       " '44': True,\n",
       " '45': True,\n",
       " '46': True,\n",
       " '47': True,\n",
       " '48': True,\n",
       " '49': True,\n",
       " '50': True,\n",
       " '51': True,\n",
       " '52': True,\n",
       " '53': True,\n",
       " '54': True,\n",
       " '55': True,\n",
       " '56': True,\n",
       " '57': True,\n",
       " '58': True,\n",
       " '59': True,\n",
       " '60': True,\n",
       " '61': True,\n",
       " '62': True,\n",
       " '63': True,\n",
       " '64': True,\n",
       " '65': True,\n",
       " '66': True,\n",
       " '67': True,\n",
       " '68': True,\n",
       " '69': True,\n",
       " '70': True,\n",
       " '71': True,\n",
       " '72': True,\n",
       " '73': True,\n",
       " '74': True,\n",
       " '75': True,\n",
       " '76': True,\n",
       " '77': True,\n",
       " '78': True,\n",
       " '79': True,\n",
       " '80': True,\n",
       " '81': True,\n",
       " '82': True,\n",
       " '83': True,\n",
       " '84': True,\n",
       " '85': True,\n",
       " '86': True,\n",
       " '87': True,\n",
       " '88': True,\n",
       " '89': True,\n",
       " '90': True,\n",
       " '91': True,\n",
       " '92': True,\n",
       " '93': True,\n",
       " '94': True,\n",
       " '95': True,\n",
       " '96': True,\n",
       " '97': True,\n",
       " '98': True,\n",
       " '99': True,\n",
       " '100': True}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d879190",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(dataset_creator_train.masks)\n",
    "masks_t = []\n",
    "for i in range(size):\n",
    "    masks_t.append(\n",
    "        np.ones((1, 1, 1)).astype(np.int64) * (i+1)\n",
    "    )\n",
    "masks_t = np.array(masks_t)\n",
    "from multiprocessing import shared_memory\n",
    "existing_shm_masks = shared_memory.SharedMemory(name=dataset_creator_train.data_shm_masks.shm_name)\n",
    "dataset_imgs_np = np.ndarray(\n",
    "    dataset_creator_train.data_shm_masks.shape, \n",
    "    dtype=dataset_creator_train.data_shm_masks.dtype, buffer=existing_shm_masks.buf\n",
    ")\n",
    "dataset_imgs_np[:] = masks_t[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16fb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fff9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b13239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eef8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(dataset_creator_test.masks)\n",
    "dataset_creator_test.masks = []\n",
    "for i in range(size):\n",
    "    dataset_creator_test.masks.append(\n",
    "        np.ones((1, 1, 1)).astype(np.int64) * (i+1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84a5000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "361it [00:13, 26.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess data...\n",
      "Create np array of imgs and masks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 36/256 [00:00<00:00, 312.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split imgs dataset...\n",
      "Start preprocess images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 884.74it/s]\n",
      "100%|██████████| 256/256 [00:00<00:00, 13700.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess masks...\n",
      "Finish preprocess!\n"
     ]
    }
   ],
   "source": [
    "dataset_creator_test = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=train_indices, cut_window=(1, 1),\n",
    "    create_shared_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95b1dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = HsiDataloaderCutter(\n",
    "    images=dataset_creator_test.images, masks=dataset_creator_test.masks,\n",
    "    preprocessing=preprocessing, \n",
    "    augmentation=test_augmentation, indices=train_indices,\n",
    "    shuffle_data=False, cut_window=(1, 1),\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1, \n",
    "    num_workers=14, pin_memory=False, prefetch_factor=2,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d5874a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b66dddaeac0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0min_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1_000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, exc_tb)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_close\u001b[0;34m(self, _close)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0m_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0m_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for in_x, target in iter(test_loader):\n",
    "    if counter % 1_000 == 0:\n",
    "        print(counter)\n",
    "    counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b042e99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 17, 1, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "89b36d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3f430f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[301908]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0623e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bcde9139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.2171]],\n",
      "\n",
      "        [[-0.2690]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.2171]],\n",
       "\n",
       "        [[-0.2690]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "print(in_x[0, :2])\n",
    "TF.rotate(in_x, -150, interpolation=T.InterpolationMode.NEAREST)[0, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f60d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
