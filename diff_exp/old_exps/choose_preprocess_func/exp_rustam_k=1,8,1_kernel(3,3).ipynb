{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12720b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/makitorch')\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/')\n",
    "\n",
    "PREFIX_INFO_PATH = '/home/rustam/hyperspecter_segmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60264d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makitorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import comet_ml\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "from Losses import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from makitorch.architectures.U2Net import U2Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsi_dataset_api import HsiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makitorch.dataloaders.HsiDataloader import HsiDataloader\n",
    "from makitorch.architectures.Unet import Unet, UnetWithFeatureSelection\n",
    "from makitorch.loss import muti_bce_loss_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "def clear_metric_calculation(final_metric, target_t, pred_t, num_classes=17):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    final_metric: torch.Tensor\n",
    "        Tensor with shape (N, C)\n",
    "    target_t: torch.Tensor or list\n",
    "        Tensor with shape (N, 1, H, W)\n",
    "    pred_t: torch.Tensor or list\n",
    "        Tensor with shape (N, C, H, W)\n",
    "    \n",
    "    \"\"\"\n",
    "    # For each image\n",
    "    final_metric_dict = dict([\n",
    "        (str(i), []) for i in range(num_classes)\n",
    "    ])\n",
    "    for metric_s, target_t_s, pred_t_s in zip(final_metric, target_t, pred_t):\n",
    "        unique_indx_target = torch.unique(target_t_s) \n",
    "        unique_indx_pred = torch.unique(pred_t_s)\n",
    "        for i in range(num_classes):\n",
    "            if i in unique_indx_target or i in unique_indx_pred:\n",
    "                final_metric_dict[str(i)].append(metric_s[i])\n",
    "    \n",
    "    mean_per_class_metric = [\n",
    "        sum(final_metric_dict[str(i)]) / len(final_metric_dict[str(i)])\n",
    "        if len(final_metric_dict[str(i)]) != 0\n",
    "        else 0.0\n",
    "        for i in range(num_classes)\n",
    "    ] \n",
    "    mean_metric = sum(mean_per_class_metric) / len(mean_per_class_metric)\n",
    "    return mean_per_class_metric, mean_metric\n",
    "\n",
    "\n",
    "def matrix2onehot(matrix, num_classes=17):\n",
    "    matrix = matrix.copy().reshape(-1)\n",
    "    one_hoted = np.zeros((matrix.size, num_classes))\n",
    "    one_hoted[np.arange(matrix.size),matrix] = 1\n",
    "    return one_hoted\n",
    "\n",
    "\n",
    "def collect_prediction_and_target(eval_loader, model):\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    for in_data_x, val_data in iter(eval_loader):\n",
    "        preds = model(in_data_x)\n",
    "        \n",
    "        target_list.append(val_data)\n",
    "        pred_list.append(preds)\n",
    "    return (torch.cat(pred_list, dim=0), \n",
    "            torch.cat(target_list, dim=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def list_target_to_onehot(target_tensor, num_classes=17):\n",
    "    one_hoted_list = []\n",
    "    for target in target_tensor:\n",
    "        target =  np.squeeze(target.cpu().detach().numpy())\n",
    "        h,w = target.shape\n",
    "        target = matrix2onehot(target, num_classes=num_classes)\n",
    "        target = target.reshape(h, w, -1)\n",
    "        target = np.transpose(target, [2, 0, 1])\n",
    "        one_hoted_list.append(target)\n",
    "    return torch.from_numpy(np.stack(one_hoted_list, axis=0))\n",
    "        \n",
    "\n",
    "def calculate_iou(pred_list, target_list, num_classes=17, loss=None):\n",
    "    res_list = []\n",
    "    loss_list = []\n",
    "    pred_as_mask_list = []\n",
    "    \n",
    "    for preds, target in zip(pred_list, target_list):\n",
    "        if loss is not None:\n",
    "            loss_list.append(\n",
    "                loss(torch.unsqueeze(preds, dim=0), torch.unsqueeze(target, dim=0)).cpu().detach().numpy()\n",
    "            )\n",
    "        else:\n",
    "            loss_list.append(None)\n",
    "        \n",
    "        preds = nn.functional.softmax(preds, dim=0).cpu().detach().numpy()\n",
    "        preds = np.squeeze(np.argmax(preds, axis=0))\n",
    "        pred_as_mask_list.append(preds)\n",
    "        \n",
    "        target = np.squeeze(target.cpu().detach().numpy())\n",
    "        \n",
    "        preds_one_hoted = matrix2onehot(preds, num_classes=num_classes)\n",
    "        target_one_hoted = matrix2onehot(target, num_classes=num_classes)\n",
    "        res = jaccard_score(target_one_hoted, preds_one_hoted, average=None, zero_division=1)\n",
    "        res_list.append(\n",
    "            res\n",
    "        )\n",
    "    \n",
    "    res_np = np.stack(res_list)\n",
    "    #res_np = res_np.mean(axis=0)\n",
    "    return res_np, loss_list, pred_as_mask_list\n",
    "\n",
    "\n",
    "def dice_loss(preds, ground_truth, eps=1e-5, dim=None, use_softmax=False, softmax_dim=1):\n",
    "    \"\"\"\n",
    "    Computes Dice loss according to the formula from:\n",
    "    V-Net: Fully Convolutional Neural Networks forVolumetric Medical Image Segmentation\n",
    "    Link to the paper: http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : tf.Tensor\n",
    "        Predicted probabilities.\n",
    "    ground_truth : tf.Tensor\n",
    "        Ground truth labels.\n",
    "    eps : float\n",
    "        Used to prevent division by zero in the Dice denominator.\n",
    "    axes : list\n",
    "        Defines which axes the dice value will be computed on. The computed dice values will be averaged\n",
    "        along the remaining axes. If None, Dice is computed on an entire batch.\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Scalar dice loss tensor.\n",
    "    \"\"\"\n",
    "    ground_truth = ground_truth.float().to(device=preds.device)\n",
    "    \n",
    "    if use_softmax:\n",
    "        preds = nn.functional.softmax(preds, dim=softmax_dim)\n",
    "    \n",
    "    numerator = preds * ground_truth\n",
    "    numerator = torch.sum(numerator, dim=dim)\n",
    "\n",
    "    p_squared = torch.square(preds)\n",
    "    p_squared = torch.sum(p_squared, dim=dim)\n",
    "    # ground_truth is not squared to avoid unnecessary computation.\n",
    "    # 0^2 = 0\n",
    "    # 1^2 = 1\n",
    "    g_squared = torch.sum(torch.square(ground_truth), dim=dim)\n",
    "    denominator = p_squared + g_squared + eps\n",
    "\n",
    "    dice = 2 * numerator / denominator\n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if getattr(m, 'bias') is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f73338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightConstraint(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'weight'):\n",
    "            w=module.weight.data\n",
    "            w=w.clamp(0, 1)\n",
    "            module.weight.data=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a84e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, fs_weight, preds, mask):\n",
    "        return self.ce(preds, mask) + torch.sum(1 - (torch.abs(fs_weight) / 0.99 - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NnModel(pl.LightningModule):\n",
    "    def __init__(self, model, loss, experiment=None, enable_image_logging=True):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.experiment = experiment\n",
    "        self.enable_image_logging = enable_image_logging\n",
    "        #self.weight_contraint_function = WeightConstraint()\n",
    "\n",
    "    def _custom_histogram_adder(self):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name,params,self.current_epoch)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.94)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        img, mask = train_batch\n",
    "        preds = self.model(img)\n",
    "        loss = self.loss(preds, mask)\n",
    "        self.log('train_loss', loss)\n",
    "        if self.experiment is not None:\n",
    "            self.experiment.log_metric(\"train_loss\", loss, epoch=self.current_epoch, step=self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return batch\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print('Size epoch end input: ', len(outputs))\n",
    "        \n",
    "        pred_tensor, target_tensor = collect_prediction_and_target(outputs, self.model)\n",
    "        target_one_hotted_tensor = list_target_to_onehot(target_tensor)\n",
    "        dice_loss_val = dice_loss(pred_tensor, target_one_hotted_tensor, dim=[0, 2, 3], use_softmax=True, softmax_dim=1)\n",
    "        metric, loss_list, pred_as_mask_list = calculate_iou(pred_tensor, target_tensor, loss=self.loss)\n",
    "        \n",
    "        for batch_idx, (loss_s, metric_s, target_s, pred_s) in enumerate(zip(loss_list, metric, target_tensor, pred_as_mask_list)):\n",
    "            if self.enable_image_logging:\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                sns.heatmap(pred_s, ax=ax1, vmin=0, vmax=17)\n",
    "                sns.heatmap(target_s.cpu().detach().numpy(), ax=ax2, vmin=0, vmax=17)\n",
    "                fig.savefig('temp_fig.png')\n",
    "                plt.close(fig)\n",
    "\n",
    "    #             trainer.logger.experiment.log_histogram_3d(\n",
    "    #                 self.model.features_selection.weight.detach().cpu().numpy(),\n",
    "    #                 name='band-selection layer',\n",
    "    #                 step=self.global_step\n",
    "    #             )\n",
    "                if self.experiment is not None:\n",
    "                    # For Comet logger\n",
    "                    self.experiment.log_image(\n",
    "                        'temp_fig.png', name=f'{batch_idx}', \n",
    "                        overwrite=False, step=self.global_step\n",
    "                    )\n",
    "            \n",
    "            d = {f'iou_{i}': iou for i, iou in enumerate(metric_s)}\n",
    "            \n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metrics(d, epoch=self.current_epoch)\n",
    "            else:\n",
    "                print(d)\n",
    "                \n",
    "            d = {f'loss_image_{batch_idx}': torch.tensor(loss_s, dtype=torch.float) }\n",
    "            if self.experiment is not None:\n",
    "                self.experiment.log_metrics(d, epoch=self.current_epoch)\n",
    "            else:\n",
    "                print(d)\n",
    "        if self.experiment is not None:\n",
    "            # Add confuse matrix\n",
    "            self.experiment.log_confusion_matrix(\n",
    "                target_tensor.cpu().detach().numpy().reshape(-1), \n",
    "                np.asarray(pred_as_mask_list).reshape(-1)\n",
    "            )\n",
    "            \n",
    "        mean_per_class_metric, mean_metric = clear_metric_calculation(metric, target_tensor, pred_tensor)\n",
    "        mean_dice_loss_per_class_dict = {\n",
    "            f\"mean_dice_loss_per_class_{i}\": torch.tensor(d_l, dtype=torch.float)\n",
    "            for i, d_l in enumerate(dice_loss_val)\n",
    "        }\n",
    "        mean_dice_loss_dict = {\n",
    "            f\"mean_dice_loss\": torch.tensor(dice_loss_val.mean(), dtype=torch.float)\n",
    "        }\n",
    "        mean_iou_class_dict = {\n",
    "            f\"mean_iou_class_{i}\": torch.tensor(iou, dtype=torch.float)\n",
    "            for i, iou in enumerate(mean_per_class_metric)\n",
    "        }\n",
    "        mean_iou_dict = {\n",
    "            \"mean_iou\": torch.tensor(mean_metric, dtype=torch.float),\n",
    "        }\n",
    "        mean_loss_dict = {\n",
    "            \"mean_loss\": torch.tensor(np.asarray(loss_list).mean(), dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        # Log this metric in order to save checkpoint of experements\n",
    "        self.log_dict(mean_iou_dict)\n",
    "        \n",
    "        if self.experiment is not None:\n",
    "        \n",
    "            self.experiment.log_metrics(\n",
    "                mean_dice_loss_per_class_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_dice_loss_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_iou_class_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_iou_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_loss_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "        else:\n",
    "            print(mean_dice_loss_per_class_dict)\n",
    "            print(mean_dice_loss_dict)\n",
    "            print(mean_iou_class_dict)\n",
    "            print(mean_iou_dict)\n",
    "            print(mean_loss_dict)\n",
    "            print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_explained_variance = np.load(f'{PREFIX_INFO_PATH}/PcaExplainedVariance_.npy')\n",
    "pca_mean = np.load(f'{PREFIX_INFO_PATH}/PcaMean.npy')\n",
    "pca_components = np.load(f'{PREFIX_INFO_PATH}/PcaComponents.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transformation(x):\n",
    "    x_t = np.reshape(x, (x.shape[0], -1)) # (C, H, W) -> (C, H * W)\n",
    "    x_t = np.swapaxes(x_t, 0, 1) # (C, H * W) -> (H * W, C)\n",
    "    x_t = x_t - pca_mean\n",
    "    x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "    return np.reshape(x_t, (x.shape[1], x.shape[2], pca_components.shape[0])).astype(np.float32) # (H, W, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_old(imgs, masks):\n",
    "    target_size = (256, 256)\n",
    "    _images = [image.resize(target_size,Image.BILINEAR)\n",
    "                   for image in imgs]\n",
    "    _masks = [mask.resize(target_size, Image.BILINEAR) for mask in masks]\n",
    "    return _images, _masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55211d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mask(mask):\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "\n",
    "    erosion = cv2.erode(mask, kernel, iterations = 1)\n",
    "    dilation = cv2.dilate(erosion, kernel,iterations = 8)\n",
    "    mask_filtered = cv2.erode(dilation, kernel, iterations = 1)\n",
    "    return mask_filtered\n",
    "\n",
    "\n",
    "def preprocessing(imgs, masks):\n",
    "    with open(f'{PREFIX_INFO_PATH}/data_standartization_params.json', 'r') as f:\n",
    "        data_standartization_params = json.load(f)\n",
    "    mean = data_standartization_params.get('means')\n",
    "    std = data_standartization_params.get('stds')\n",
    "    def standartization(img):\n",
    "        return np.array((img - mean) / std, dtype=np.float32)\n",
    "    _images = [pca_transformation(image) for image in imgs]\n",
    "    _images = [standartization(image) for image in _images]\n",
    "    _masks = [\n",
    "        np.expand_dims(\n",
    "            preprocess_mask(cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY).astype(np.uint8))\n",
    "            ,0\n",
    "        ).astype(np.int64)\n",
    "        for mask in masks\n",
    "    ]\n",
    "    return _images, _masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69079a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_augmentation(image, mask):\n",
    "    image = TF.to_tensor(image)\n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    mask = torch.from_numpy(mask)\n",
    "    \n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16432260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(image, mask):\n",
    "    image = TF.to_tensor(image)\n",
    "    mask = torch.from_numpy(mask)\n",
    "    angle = T.RandomRotation.get_params((-30, 30))\n",
    "    image = TF.rotate(image, angle, interpolation=T.InterpolationMode.BILINEAR)\n",
    "    mask = TF.rotate(mask, angle, interpolation=T.InterpolationMode.NEAREST)\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "        mask = TF.hflip(mask)\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "        mask = TF.vflip(mask)\n",
    "    \n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random = np.random.permutation(np.arange(384))\n",
    "# test_indices = random[310:]\n",
    "# train_indices = random[:310]\n",
    "\n",
    "test_indices = np.load(f'{PREFIX_INFO_PATH}/test_indices.npy')\n",
    "train_indices = np.load(f'{PREFIX_INFO_PATH}/train_indices.npy')\n",
    "path = '/raid/rustam/hyperspectral_dataset/cropped_hsi_data'\n",
    "\n",
    "dataset_train = HsiDataloader(\n",
    "    path, preprocessing=preprocessing, \n",
    "    augmentation=augmentation, indices=train_indices,\n",
    "    shuffle_data=True\n",
    ")\n",
    "dataset_test = HsiDataloader(path, preprocessing=preprocessing, augmentation=test_augmentation, indices=test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySuperNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f=17, out_f=17):\n",
    "        super().__init__()\n",
    "        self.bn_start = nn.BatchNorm2d(in_f)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_f, in_f * 4, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(in_f * 4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_f * 4, in_f * 8, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(in_f * 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_f * 8, in_f * 4, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(in_f * 4)\n",
    "        self.act3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_f * 4, in_f, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(in_f)\n",
    "        self.act4 = nn.ReLU()\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_f, out_f, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.bn_start(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a17000",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MySuperNet(17, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3705d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7788ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(torch.randn(1, 17, 512, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3ebae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comet_exp = comet_ml.Experiment(\n",
    "    api_key=\"your-key\",\n",
    "    workspace=\"your-workspace\",  # Optional\n",
    "    project_name=\"your-project-name\",  # Optional\n",
    ")\n",
    "name_exp = \"Choose preprocess//lower arch//50ep//W PCA.//RustamPreprocess(k=1,8,1_kernel(3,3))//makiloss//gamma=5.5/balance=2\"\n",
    "comet_exp.set_name(name_exp)\n",
    "#logger = TensorBoardLogger(\n",
    "#    'logs/'\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLossCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    copy from: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/FocalLoss/FocalLoss.py\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
    "    :param num_class:\n",
    "    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
    "    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
    "                    focus on hard misclassified example\n",
    "    :param smooth: (float,double) smooth value when cross entropy\n",
    "    :param balance_index: (int) balance class index, should be specific when alpha is float\n",
    "    :param size_average: (bool, optional) By default, the losses are averaged over each loss element in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=None, gamma=5.5, balance_index=2, smooth=1e-5, size_average=False):\n",
    "        super(FocalLossCustom, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.balance_index = balance_index\n",
    "        self.smooth = smooth\n",
    "        self.size_average = size_average\n",
    "        self.cel = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        if self.smooth is not None:\n",
    "            if self.smooth < 0 or self.smooth > 1.0:\n",
    "                raise ValueError('smooth value should be in [0,1]')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        num_class = logit.shape[1]\n",
    "\n",
    "        if logit.dim() > 2:\n",
    "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            logit = logit.view(logit.size(0), logit.size(1), -1)\n",
    "            # N,C,m -> N,m,C\n",
    "            logit = logit.permute(0, 2, 1).contiguous()\n",
    "            # N,m,C -> N,m*C\n",
    "            logit = logit.view(-1, logit.size(-1))\n",
    "        target = torch.squeeze(target, 1)\n",
    "        target = target.view(-1, 1)\n",
    "        \n",
    "        ce_loss = self.cel(logit, target.view(-1))\n",
    "        train_conf = self.softmax(logit)\n",
    "        \n",
    "        idx = target.cpu().long()\n",
    "        one_hot_labels  = torch.FloatTensor(target.size(0), num_class).zero_()\n",
    "        one_hot_labels  = one_hot_labels.scatter_(1, idx, 1)\n",
    "        if one_hot_labels.device != logit.device:\n",
    "            one_hot_labels = one_hot_labels.to(logit.device)\n",
    "        \n",
    "        filtered_conf = train_conf * one_hot_labels\n",
    "        sparce_conf, _ = torch.max(filtered_conf, dim=-1)\n",
    "        loss = torch.pow((torch.ones_like(sparce_conf) - sparce_conf), self.gamma) * ce_loss\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        if not self.size_average:\n",
    "            # Norm by positive\n",
    "            num_positive = torch.sum(target != self.balance_index)\n",
    "            loss = loss.sum() / (num_positive + 1e-10)\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72220c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = NnModel(net, muti_bce_loss_fusion, enable_image_logging=True)\n",
    "model = NnModel(net, FocalLossCustom(), experiment=comet_exp)\n",
    "\n",
    "# saves a checkpoint-file\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"pytorch_li_logs/rustam_k=1,8,1_kernel(3,3)\",\n",
    "    monitor=\"mean_iou\",\n",
    "    filename=\"model-{epoch:02d}-{mean_iou:.2f}\",\n",
    "    save_top_k=-1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=50,\n",
    "    check_val_every_n_epoch=2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=1, \n",
    "#     max_epochs=2000,\n",
    "#     check_val_every_n_epoch=2000)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "comet_exp.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ea869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf3ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1f023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
