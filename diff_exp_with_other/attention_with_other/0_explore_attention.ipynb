{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/makitorch')\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/')\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "from hsi_dataset_api import HsiDataset\n",
    "from makitorch.dataloaders.HsiDataloader import HsiDataloader\n",
    "\n",
    "from makitorch.data_tools.augmentation import DataAugmentator\n",
    "from makitorch.data_tools.augmentation import BaseDataAugmentor\n",
    "from makitorch.data_tools.preprocessing import BaseDataPreprocessor\n",
    "from makitorch.data_tools.preprocessing import DataPreprocessor\n",
    "from makitorch.dataset_remapper import DatasetRemapper\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "from Losses import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from multiprocessing.dummy import Pool\n",
    "from multiprocessing import shared_memory\n",
    "import seaborn as sns\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATA_SHAPE = (128, 128)\n",
    "INPUT_DIM = 17\n",
    "NUM_CLASSES = None\n",
    "PATH_DATA = '/raid/rustam/hyperspectral_dataset/new_cropped_hsi_data'\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "PATH_TO_SAVE = '/raid/rustam/hyperspectral_dataset/diff_exp_with_other__attention_with_other/result_masks_0'\n",
    "\n",
    "ATT_EXP = 'AttentionV11'\n",
    "WO_ATT_EXP = 'AttentionV11-WO'\n",
    "SW_ATT_EXP = 'SW-AttentionV11'\n",
    "SW_WO_ATT_EXP = 'SW-AttentionV11-WO'\n",
    "\n",
    "\n",
    "id2exp_name = {\n",
    "    '949923586': 'AttentionV11 kfold0 R1',\n",
    "    '121120867': 'AttentionV11 kfold0 R2',\n",
    "    '847352398': 'AttentionV11 kfold0 R3',\n",
    "    '119260030': 'AttentionV11 kfold0 R4',\n",
    "    '618375711': 'AttentionV11-WO kfold0 R1',\n",
    "    '898124683': 'AttentionV11-WO kfold0 R2',\n",
    "    '109979765': 'AttentionV11-WO kfold0 R3',\n",
    "    '633351465': 'AttentionV11-WO kfold0 R4',\n",
    "    '795994918': 'SW-AttentionV11 kfold0 R1',\n",
    "    '596305665': 'SW-AttentionV11 kfold0 R2',\n",
    "    '344737891': 'SW-AttentionV11 kfold0 R3',\n",
    "    '85837259':  'SW-AttentionV11 kfold0 R4',\n",
    "    '782108417': 'SW-AttentionV11-WO kfold0 R1',\n",
    "    '783196294': 'SW-AttentionV11-WO kfold0 R2',\n",
    "    '479759236': 'SW-AttentionV11-WO kfold0 R3',\n",
    "    '955604717': 'SW-AttentionV11-WO kfold0 R4',\n",
    "\n",
    "    '279928761': 'AttentionV11 kfold1 R1',\n",
    "    '996122000': 'AttentionV11 kfold1 R2',\n",
    "    '312177183': 'AttentionV11 kfold1 R3',\n",
    "    '663225523': 'AttentionV11 kfold1 R4',\n",
    "    '930756249': 'AttentionV11-WO kfold1 R1',\n",
    "    '946444593': 'AttentionV11-WO kfold1 R2',\n",
    "    '527159228': 'AttentionV11-WO kfold1 R3',\n",
    "    '27327763':  'AttentionV11-WO kfold1 R4',\n",
    "    '373915631': 'SW-AttentionV11 kfold1 R1',\n",
    "    '812373490': 'SW-AttentionV11 kfold1 R2',\n",
    "    '927094798': 'SW-AttentionV11 kfold1 R3',\n",
    "    '784767717': 'SW-AttentionV11 kfold1 R4',\n",
    "    '829169':    'SW-AttentionV11-WO kfold1 R1',\n",
    "    '970298918': 'SW-AttentionV11-WO kfold1 R2',\n",
    "    '24035521':  'SW-AttentionV11-WO kfold1 R3',\n",
    "    '862357190': 'SW-AttentionV11-WO kfold1 R4',\n",
    "\n",
    "    '665848744': 'AttentionV11 kfold2 R1',\n",
    "    '527008414': 'AttentionV11 kfold2 R2',\n",
    "    '962896694': 'AttentionV11 kfold2 R3',\n",
    "    '257257676': 'AttentionV11 kfold2 R4',\n",
    "    '314015568': 'AttentionV11-WO kfold2 R1',\n",
    "    '410030650': 'AttentionV11-WO kfold2 R2',\n",
    "    '942004738': 'AttentionV11-WO kfold2 R3',\n",
    "    '437994589': 'AttentionV11-WO kfold2 R4',\n",
    "    '747074509': 'SW-AttentionV11 kfold2 R1',\n",
    "    '983492227': 'SW-AttentionV11 kfold2 R2',\n",
    "    '238334469': 'SW-AttentionV11 kfold2 R3',\n",
    "    '826242444': 'SW-AttentionV11 kfold2 R4',\n",
    "    '443269504': 'SW-AttentionV11-WO kfold2 R1',\n",
    "    '259831776': 'SW-AttentionV11-WO kfold2 R2',\n",
    "    '347933396': 'SW-AttentionV11-WO kfold2 R3',\n",
    "    '241122960': 'SW-AttentionV11-WO kfold2 R4',\n",
    "}\n",
    "\n",
    "exp_name2id = dict([(v, k) for k, v in id2exp_name.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555928f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ----------------------------------------------------------------\n",
    "#                           MODEL\n",
    "# ----------------------------------------------------------------\n",
    "#\n",
    "# Paper must read:\n",
    "# https://arxiv.org/pdf/1904.11492.pdf\n",
    "#\n",
    "# Original paper:\n",
    "# https://arxiv.org/pdf/1906.02849.pdf\n",
    "#\n",
    "# Github:\n",
    "# https://github.com/sinAshish/Multi-Scale-Attention\n",
    "#\n",
    "\n",
    "\n",
    "class PAM_Module(nn.Module):\n",
    "    \"\"\" \n",
    "    Position attention module\n",
    "\n",
    "    \"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim, dim_reduse: int = 8):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim // dim_reduse, \n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim // dim_reduse,\n",
    "             kernel_size=1\n",
    "        )\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, \n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attention\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    \"\"\" \n",
    "    Channel attention module\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "       \n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy # swap?\n",
    "        attention = self.softmax(energy_new) # Sigmoid?\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attention\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MySuperAttentionNetLittleInput(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f=237, out_f=NUM_CLASSES, *args):\n",
    "        super().__init__()\n",
    "        #self.bn_start = nn.BatchNorm3d(in_f)\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            # (N, in_f, 128, 128)\n",
    "            nn.Conv2d(in_f, 64, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            # (N, 128, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Output (N, 128, 32, 32)\n",
    "        self.cam_module = CAM_Module(128)\n",
    "        # Output (N, 128, 32, 32)\n",
    "        self.pam_module = PAM_Module(128, dim_reduse=8)\n",
    "        # Concat out_f and out_f = out_f * 2\n",
    "        self.final_backbone = nn.Sequential(\n",
    "            nn.Conv2d(128 * 3, 128, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            # (N, 128, 64, 64)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            # (N, 64, 128, 128)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, out_f, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            # (N, out_f, 128, 128)\n",
    "            nn.BatchNorm2d(out_f),\n",
    "            nn.ReLU(),\n",
    "            # Final conv\n",
    "            nn.Conv2d(out_f, out_f, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        cam_x = self.cam_module(x)\n",
    "        pam_x = self.pam_module(x)\n",
    "\n",
    "        x = torch.cat([cam_x, pam_x, x], dim=1)\n",
    "        x = self.final_backbone(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "     \n",
    "class MySuperNetLittleInput(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f=237, out_f=NUM_CLASSES, *args):\n",
    "        super().__init__()\n",
    "        #self.bn_start = nn.BatchNorm3d(in_f)\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            # (N, in_f, 128, 128)\n",
    "            nn.Conv2d(in_f, 64, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            # (N, 128, 64, 64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128 * 3, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # (N, 128 * 3, 32, 32)\n",
    "            nn.BatchNorm2d(128 * 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.final_backbone = nn.Sequential(\n",
    "            nn.Conv2d(128 * 3, 128, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            # (N, 128, 32, 32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            # (N, 128, 64, 64)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            # (N, 64, 128, 128)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, out_f, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            # (N, out_f, 128, 128)\n",
    "            nn.BatchNorm2d(out_f),\n",
    "            nn.ReLU(),\n",
    "            # Final conv\n",
    "            nn.Conv2d(out_f, out_f, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.final_backbone(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43562df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShmData:\n",
    "\n",
    "    def __init__(self, shm_name, shape, dtype):\n",
    "        self.shm_name = shm_name\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "class HsiDataloaderCutter(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images, masks,\n",
    "            augmentation: Optional[Union[DataAugmentator, Callable]] = BaseDataAugmentor(),\n",
    "            shuffle_data=False,\n",
    "            cut_window=(8, 8),\n",
    "            map_mask_to_class=False,\n",
    "            shared_memory_imgs_data: ShmData = None,\n",
    "            shared_memory_masks_data: ShmData = None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.shuffle_data = shuffle_data\n",
    "        self.augmentation = augmentation\n",
    "        if cut_window is not None and cut_window[0] == 1 and cut_window[1] == 1:\n",
    "            self.ignore_image_augs = True\n",
    "        else:\n",
    "            self.ignore_image_augs = False\n",
    "        self.cut_window = cut_window\n",
    "        self.map_mask_to_class = map_mask_to_class\n",
    "        self.shared_memory_imgs_data = shared_memory_imgs_data\n",
    "        self.shared_memory_masks_data = shared_memory_masks_data\n",
    "        \n",
    "        self.shm_imgs: shared_memory.SharedMemory = None\n",
    "        self.shm_masks: shared_memory.SharedMemory = None\n",
    "\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert self.images is not None and self.masks is not None\n",
    "        if self.shuffle_data:\n",
    "            self.images, self.masks = shuffle(self.images, self.masks)\n",
    "        \n",
    "        for image, mask in zip(self.images, self.masks):\n",
    "            yield self.augmentation(\n",
    "                image, mask, \n",
    "                map_mask_to_class=self.map_mask_to_class,\n",
    "                ignore_image_augs=self.ignore_image_augs\n",
    "            )\n",
    "\n",
    "\n",
    "def cut_into_parts_model_input(\n",
    "        image: np.ndarray, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    image_parts_list = []\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            img_part = image[:, :,  \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "\n",
    "            image_parts_list.append(img_part)\n",
    "    return image_parts_list\n",
    "\n",
    "\n",
    "def merge_parts_into_single_mask(\n",
    "        preds, shape, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    pred_mask = torch.zeros(\n",
    "        shape,\n",
    "        dtype=preds.dtype, device=preds.device\n",
    "    )\n",
    "    counter = 0\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            pred_mask[:, :,  \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ] = preds[counter]\n",
    "            counter += 1\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def collect_prediction_and_target(\n",
    "        eval_loader, model, cut_window=(8, 8), \n",
    "        image_shape=(512, 512), num_classes=NUM_CLASSES,\n",
    "        divided_batch=2):\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    for in_data_x, val_data in iter(eval_loader):\n",
    "        batch_size = in_data_x.shape[0]\n",
    "        # We will cut image into peases and stack it into single BIG batch\n",
    "        h_win, w_win = cut_window\n",
    "        h_parts, w_parts = image_shape[1] // w_win, image_shape[0] // h_win\n",
    "        in_data_x_parts_list = cut_into_parts_model_input(\n",
    "            in_data_x, h_parts=h_parts, \n",
    "            w_parts=w_parts, h_win=h_win, w_win=w_win\n",
    "        )\n",
    "        in_data_x_batch = torch.cat(in_data_x_parts_list, dim=0) # (N, 17, 1, 1)\n",
    "        # Make predictions\n",
    "        part_divided = len(in_data_x_batch) // divided_batch\n",
    "        pred_batch_list = []\n",
    "        for b_i in range(part_divided):\n",
    "            if b_i == (part_divided - 1):\n",
    "                # last\n",
    "                single_batch = in_data_x_batch[b_i * divided_batch:]\n",
    "            else:\n",
    "                single_batch = in_data_x_batch[b_i * divided_batch: (b_i+1) * divided_batch]\n",
    "            # Make predictions\n",
    "            preds = model(single_batch) # (divided_batch, num_classes)\n",
    "            pred_batch_list.append(preds)\n",
    "        preds = torch.cat(pred_batch_list, dim=0) \n",
    "        # Create full image again from peases\n",
    "        pred_mask = merge_parts_into_single_mask(\n",
    "            preds=preds, shape=(batch_size, num_classes, image_shape[0], image_shape[1]), \n",
    "            h_parts=h_parts, w_parts=w_parts, h_win=h_win, w_win=w_win\n",
    "        )\n",
    "        target_list.append(val_data)\n",
    "        pred_list.append(pred_mask)\n",
    "    return (torch.cat(pred_list, dim=0), \n",
    "            torch.cat(target_list, dim=0)\n",
    "    )\n",
    "        \n",
    "\n",
    "def calculate_iou(pred_list, target_list, num_classes=NUM_CLASSES):\n",
    "    res_list = []\n",
    "    \n",
    "    for preds, target in zip(pred_list, target_list):\n",
    "        # preds - (num_classes, H, W)\n",
    "        preds = preds.detach()\n",
    "        # target - (H, W)\n",
    "        target = target.detach()\n",
    "\n",
    "        preds = nn.functional.softmax(preds, dim=0)\n",
    "        preds = torch.argmax(preds, dim=0)\n",
    "        \n",
    "        preds_one_hoted = torch.nn.functional.one_hot(preds, num_classes).view(-1, num_classes).cpu()\n",
    "        target_one_hoted = torch.nn.functional.one_hot(target, num_classes).view(-1, num_classes).cpu()\n",
    "        res = jaccard_score(target_one_hoted, preds_one_hoted, average=None, zero_division=1)\n",
    "        res_list.append(\n",
    "            res\n",
    "        )\n",
    "    \n",
    "    res_np = np.stack(res_list)\n",
    "    #res_np = res_np.mean(axis=0)\n",
    "    return res_np\n",
    "\n",
    "\n",
    "def dice_loss(preds, ground_truth, eps=1e-5, dim=None, use_softmax=False, softmax_dim=1):\n",
    "    \"\"\"\n",
    "    Computes Dice loss according to the formula from:\n",
    "    V-Net: Fully Convolutional Neural Networks forVolumetric Medical Image Segmentation\n",
    "    Link to the paper: http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : tf.Tensor\n",
    "        Predicted probabilities.\n",
    "    ground_truth : tf.Tensor\n",
    "        Ground truth labels.\n",
    "    eps : float\n",
    "        Used to prevent division by zero in the Dice denominator.\n",
    "    axes : list\n",
    "        Defines which axes the dice value will be computed on. The computed dice values will be averaged\n",
    "        along the remaining axes. If None, Dice is computed on an entire batch.\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Scalar dice loss tensor.\n",
    "    \"\"\"\n",
    "    ground_truth = ground_truth.float().to(device=preds.device)\n",
    "    \n",
    "    if use_softmax:\n",
    "        preds = nn.functional.softmax(preds, dim=softmax_dim)\n",
    "    \n",
    "    numerator = preds * ground_truth\n",
    "    numerator = torch.sum(numerator, dim=dim)\n",
    "\n",
    "    p_squared = torch.square(preds)\n",
    "    p_squared = torch.sum(p_squared, dim=dim)\n",
    "    # ground_truth is not squared to avoid unnecessary computation.\n",
    "    # 0^2 = 0\n",
    "    # 1^2 = 1\n",
    "    g_squared = torch.sum(torch.square(ground_truth), dim=dim)\n",
    "    denominator = p_squared + g_squared + eps\n",
    "\n",
    "    dice = 2 * numerator / denominator\n",
    "    return 1 - dice\n",
    "\n",
    "\n",
    "def take_pred_masks(preds):\n",
    "    preds = nn.functional.softmax(preds, dim=1)\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if getattr(m, 'bias') is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ----------------------------------------------------------------\n",
    "#                  PYTORCH LIGHTNING MODEL\n",
    "# ----------------------------------------------------------------\n",
    "#\n",
    "\n",
    "class NnModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self, model, loss,\n",
    "            T_0=10, T_mult=2, experiment=None, enable_image_logging=True,\n",
    "            lr=1e-3, lr_list=None, epoch_list=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.experiment = experiment\n",
    "        self.enable_image_logging = enable_image_logging\n",
    "\n",
    "        if lr_list is not None and epoch_list is not None:\n",
    "            if len(lr_list) != len(epoch_list):\n",
    "                raise ValueError(\n",
    "                    f\"lr_list={lr_list} and epoch_list={epoch_list}\"+\\\n",
    "                    \" must be arrays of same length\"\n",
    "                )\n",
    "            print(\n",
    "                f'Using dynamic lr with next setup: lr_list={lr_list} epoch_list={sorted(epoch_list)}.\\n+'+\\\n",
    "                'Epoch list is sorted by increasing.'\n",
    "            )\n",
    "            # Further lr/epoch value will be added/deleted into list\n",
    "            # Make copy to make sure that original data is safe\n",
    "            lr_list = copy.deepcopy(lr_list)\n",
    "            epoch_list = copy.deepcopy(epoch_list)\n",
    "\n",
    "            self.lr_list = lr_list\n",
    "            self.epoch_list = sorted(epoch_list)\n",
    "            self.is_lr_must_change = True\n",
    "        else:\n",
    "            self.is_lr_must_change = False\n",
    "\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        # Stuff here - for change learning rate on a fly\n",
    "        if self.is_lr_must_change and len(self.epoch_list) != 0:\n",
    "            # Check, current epoch bigger than epoch on which must be updated lr\n",
    "            if self.current_epoch >= self.epoch_list[0]:\n",
    "                # Update optimizer\n",
    "                self.lr = self.lr_list[0]\n",
    "                self.trainer.accelerator.setup_optimizers(self.trainer)\n",
    "                # Clear used variables\n",
    "                del self.epoch_list[0]\n",
    "                del self.lr_list[0]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Change lr after some epoch, original idea from:\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/issues/3095\n",
    "        # But here I do something different and its more what I like\n",
    "        print(f'Init optimizer with params: lr={self.lr}, T_0={self.T_0}, T_mult={self.T_mult}')\n",
    "        optimizer = optim.Adam( \n",
    "            self.parameters(), lr=self.lr\n",
    "        )\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, \n",
    "            T_0=self.T_0, T_mult=self.T_mult, eta_min=0\n",
    "        )\n",
    "        return { \"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler }\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        img, mask = train_batch\n",
    "        preds = self.model(img) # (N, C, H, W)\n",
    "        loss = self.loss(preds, mask) # (N, H, W)\n",
    "        self.log('train_loss', loss)\n",
    "        if self.experiment is not None:\n",
    "            self.experiment.log_metric(\"train_loss\", loss, epoch=self.current_epoch, step=self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return batch\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print('VALIDATION: Size epoch end input with data n=', len(outputs))\n",
    "        \n",
    "        def create_big_image(tensor, single_elem_shape):\n",
    "            shape = tensor.shape\n",
    "            tensor = torch.cat([\n",
    "                t_s.view(*single_elem_shape)\n",
    "                for t_s in tensor\n",
    "            ], dim=-1)\n",
    "            return tensor\n",
    "        # (batch_size, NUM_CLASSES, H, W) | (batch_size, H, W)\n",
    "        pred_tensor, target_tensor = collect_prediction_and_target(outputs, self.model)\n",
    "        pred_as_mask = take_pred_masks(pred_tensor) # (batch_size, H, W)\n",
    "        pred_big_tensor = create_big_image(\n",
    "            pred_tensor, \n",
    "            single_elem_shape=[1, NUM_CLASSES, DATA_SHAPE[0], DATA_SHAPE[1]]\n",
    "        )\n",
    "        target_big_tensor = create_big_image(\n",
    "            target_tensor, \n",
    "            single_elem_shape=[1, DATA_SHAPE[0], DATA_SHAPE[1]]\n",
    "        )\n",
    "\n",
    "        target_one_hotted_tensor = torch.nn.functional.one_hot(\n",
    "            target_big_tensor, NUM_CLASSES # Num classes\n",
    "        )\n",
    "        # (N, H, W, C) --> (N, C, H, W)\n",
    "        target_one_hotted_tensor = target_one_hotted_tensor.permute(0, -1, 1, 2)\n",
    "        dice_loss_val = dice_loss(\n",
    "            pred_big_tensor, target_one_hotted_tensor, \n",
    "            dim=[0, 2, 3], use_softmax=True, softmax_dim=1,\n",
    "        )\n",
    "        metric = calculate_iou(pred_big_tensor, target_big_tensor)\n",
    "        \n",
    "        if self.enable_image_logging:\n",
    "            for batch_indx, (target_s, pred_s) in enumerate(zip(target_tensor, pred_as_mask)):\n",
    "                target_s = target_s.squeeze()\n",
    "                pred_s = pred_s.squeeze()\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                sns.heatmap(pred_s.cpu().detach().numpy(), ax=ax1, vmin=0, vmax=NUM_CLASSES)\n",
    "                sns.heatmap(target_s.cpu().detach().numpy(), ax=ax2, vmin=0, vmax=NUM_CLASSES)\n",
    "                fig.savefig(f'temp_fig_{GPU_ID}.png')\n",
    "                plt.close(fig)\n",
    "\n",
    "                if self.experiment is not None and self.current_epoch != 0:\n",
    "                    self.experiment.log_image(\n",
    "                        f'temp_fig_{GPU_ID}.png', name=f'{batch_indx}', \n",
    "                        overwrite=False, step=self.global_step\n",
    "                    )\n",
    "                # Sometimes - send too much images to comet - slow training\n",
    "                # So, for training time skip some samples - its better to do this after \n",
    "                # training on the best model\n",
    "                if batch_indx == NUMBER_RESULTS_TO_PLOT:\n",
    "                    break\n",
    "\n",
    "        if self.experiment is not None and self.current_epoch != 0:\n",
    "            self.experiment.log_confusion_matrix(\n",
    "                target_tensor.cpu().detach().numpy().reshape(-1), \n",
    "                torch.stack(\n",
    "                    [elem.cpu() for elem in pred_as_mask], \n",
    "                    dim=0\n",
    "                ).cpu().detach().numpy().reshape(-1)\n",
    "            )\n",
    "\n",
    "        mean_dice_loss_per_class_dict = {\n",
    "            f\"mean_dice_loss_per_class_{i}\": d_l.float()\n",
    "            for i, d_l in enumerate(dice_loss_val)\n",
    "        }\n",
    "        mean_dice_loss_dict = {\n",
    "            f\"mean_dice_loss\": dice_loss_val.mean().float()\n",
    "        }\n",
    "        mean_iou_class_dict = {\n",
    "            f\"mean_iou_class_{i}\": torch.tensor(iou, dtype=torch.float)\n",
    "            for i, iou in enumerate(metric.mean(axis=0))\n",
    "        }\n",
    "        mean_iou_dict = {\n",
    "            \"mean_iou\": float(metric.mean()),\n",
    "        }\n",
    "        \n",
    "        # Log will save stuff in comet\n",
    "        self.log_dict(mean_iou_dict)\n",
    "        \n",
    "        if self.experiment is not None and self.current_epoch != 0:\n",
    "        \n",
    "            self.experiment.log_metrics(\n",
    "                mean_dice_loss_per_class_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_dice_loss_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_iou_class_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_iou_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "        else:\n",
    "            print(mean_dice_loss_per_class_dict)\n",
    "            print(mean_dice_loss_dict)\n",
    "            print(mean_iou_class_dict)\n",
    "            print(mean_iou_dict)\n",
    "            print('---------------------------------')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return batch\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # This function almost same as validation_epoch_end - but its only for test data\n",
    "        # TODO: Make it as one function?\n",
    "        print('TEST: Performe test with data n=', len(outputs))\n",
    "        \n",
    "        def create_big_image(tensor, single_elem_shape):\n",
    "            shape = tensor.shape\n",
    "            tensor = torch.cat([\n",
    "                t_s.view(*single_elem_shape)\n",
    "                for t_s in tensor\n",
    "            ], dim=-1)\n",
    "            return tensor\n",
    "        # (batch_size, NUM_CLASSES, H, W) | (batch_size, H, W)\n",
    "        pred_tensor, target_tensor = collect_prediction_and_target(outputs, self)\n",
    "        pred_as_mask = take_pred_masks(pred_tensor) # (batch_size, H, W)\n",
    "        pred_big_tensor = create_big_image(\n",
    "            pred_tensor, \n",
    "            single_elem_shape=[1, NUM_CLASSES, DATA_SHAPE[0], DATA_SHAPE[1]]\n",
    "        )\n",
    "        target_big_tensor = create_big_image(\n",
    "            target_tensor, \n",
    "            single_elem_shape=[1, DATA_SHAPE[0], DATA_SHAPE[1]]\n",
    "        )\n",
    "\n",
    "        target_one_hotted_tensor = torch.nn.functional.one_hot(\n",
    "            target_big_tensor, NUM_CLASSES # Num classes\n",
    "        )\n",
    "        # (N, H, W, C) --> (N, C, H, W)\n",
    "        target_one_hotted_tensor = target_one_hotted_tensor.permute(0, -1, 1, 2)\n",
    "        dice_loss_val = dice_loss(\n",
    "            pred_big_tensor, target_one_hotted_tensor, \n",
    "            dim=[0, 2, 3], use_softmax=True, softmax_dim=1,\n",
    "        )\n",
    "        metric = calculate_iou(pred_big_tensor, target_big_tensor)\n",
    "        \n",
    "        if self.enable_image_logging:\n",
    "            for batch_indx, (target_s, pred_s) in enumerate(zip(target_tensor, pred_as_mask)):\n",
    "                target_s = target_s.squeeze()\n",
    "                pred_s = pred_s.squeeze()\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                sns.heatmap(pred_s.cpu().detach().numpy(), ax=ax1, vmin=0, vmax=NUM_CLASSES)\n",
    "                sns.heatmap(target_s.cpu().detach().numpy(), ax=ax2, vmin=0, vmax=NUM_CLASSES)\n",
    "                fig.savefig(f'test_unknown_temp_fig_{GPU_ID}.png')\n",
    "                plt.close(fig)\n",
    "\n",
    "                if self.experiment is not None and self.current_epoch != 0:\n",
    "                    self.experiment.log_image(\n",
    "                        f'test_unknown_temp_fig_{GPU_ID}.png', \n",
    "                        name=f'unknown_{batch_indx}', \n",
    "                        overwrite=False, step=self.global_step\n",
    "                    )\n",
    "\n",
    "        if self.experiment is not None and self.current_epoch != 0:\n",
    "            self.experiment.log_confusion_matrix(\n",
    "                target_tensor.cpu().detach().numpy().reshape(-1), \n",
    "                torch.stack(\n",
    "                    [elem.cpu() for elem in pred_as_mask], \n",
    "                    dim=0\n",
    "                ).cpu().detach().numpy().reshape(-1)\n",
    "            )\n",
    "\n",
    "        mean_dice_loss_per_class_dict = {\n",
    "            f\"test_mean_dice_loss_per_class_{i}\": d_l.float()\n",
    "            for i, d_l in enumerate(dice_loss_val)\n",
    "        }\n",
    "        mean_dice_loss_dict = {\n",
    "            f\"test_mean_dice_loss\": dice_loss_val.mean().float()\n",
    "        }\n",
    "        mean_iou_class_dict = {\n",
    "            f\"test_mean_iou_class_{i}\": torch.tensor(iou, dtype=torch.float)\n",
    "            for i, iou in enumerate(metric.mean(axis=0))\n",
    "        }\n",
    "        mean_iou_dict = {\n",
    "            \"test_mean_iou\": float(metric.mean()),\n",
    "        }\n",
    "        \n",
    "        # Log will save stuff in comet\n",
    "        self.log_dict(mean_iou_dict)\n",
    "        \n",
    "        if self.experiment is not None and self.current_epoch != 0:\n",
    "        \n",
    "            self.experiment.log_metrics(\n",
    "                mean_dice_loss_per_class_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_dice_loss_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_iou_class_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "\n",
    "            self.experiment.log_metrics(\n",
    "                mean_iou_dict,\n",
    "                epoch=self.current_epoch\n",
    "            )\n",
    "        else:\n",
    "            print(mean_dice_loss_per_class_dict)\n",
    "            print(mean_dice_loss_dict)\n",
    "            print(mean_iou_class_dict)\n",
    "            print(mean_iou_dict)\n",
    "            print('---------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2892e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.njit\n",
    "def cut_into_parts(\n",
    "        image: np.ndarray, mask: np.ndarray, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    image_parts_list = []\n",
    "    mask_parts_list = []\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            img_part = image[:, \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "            mask_part = mask[\n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "\n",
    "            image_parts_list.append(img_part)\n",
    "            mask_parts_list.append(mask_part)\n",
    "    return image_parts_list, mask_parts_list\n",
    "\n",
    "\n",
    "\n",
    "class DatasetCreator:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_path: str,\n",
    "            preprocessing: Optional[Union[DataPreprocessor, Callable]] = BaseDataPreprocessor(),\n",
    "            indices = None,\n",
    "            cut_window=(8, 8),\n",
    "            map_mask_to_class=False,\n",
    "            create_shared_memory=False,\n",
    "            shuffle_then_prepared=False,\n",
    "            path_old2new: str = None):\n",
    "        self.dataset = HsiDataset(data_path)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.cut_window = cut_window\n",
    "        self.map_mask_to_class = map_mask_to_class\n",
    "        self.create_shared_memory = create_shared_memory\n",
    "        \n",
    "        if path_old2new:\n",
    "            self.old2new_mapper = DatasetRemapper(np.load(path_old2new))\n",
    "        else:\n",
    "            self.old2new_mapper = None\n",
    "\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        self._shm_imgs = None\n",
    "        self._shm_masks = None\n",
    "\n",
    "        for idx, data_point in tqdm(enumerate(self.dataset.data_iterator(opened=True, shuffle=False))):\n",
    "            if indices is not None and idx not in indices:\n",
    "                continue\n",
    "            image, mask = data_point.hsi, data_point.mask\n",
    "\n",
    "            if self.old2new_mapper:\n",
    "                _, mask = self.old2new_mapper(None, mask)\n",
    "\n",
    "            if cut_window is not None:\n",
    "                image_parts, mask_parts = self._cut_with_window(image, mask, cut_window)\n",
    "                self.images += image_parts\n",
    "                self.masks += mask_parts\n",
    "            else:\n",
    "                self.images.append(image)\n",
    "                self.masks.append(mask)\n",
    "        print(\"Preprocess data...\")\n",
    "        if self.preprocessing is not None:\n",
    "            self.images, self.masks = self.preprocessing(\n",
    "                self.images, self.masks, map_mask_to_class=map_mask_to_class\n",
    "            )\n",
    "\n",
    "        if shuffle_then_prepared:\n",
    "            self.images, self.masks = shuffle(self.images, self.masks)\n",
    "\n",
    "        # Create shared memory\n",
    "        if create_shared_memory:\n",
    "            print('Create shared memory...')\n",
    "            # First - map images and masks into np\n",
    "            self.images = np.asarray(self.images, dtype=np.float32)\n",
    "            self.masks = np.asarray(self.masks, dtype=np.int64)\n",
    "            # Imgs\n",
    "            shm_imgs = shared_memory.SharedMemory(create=True, size=self.images.nbytes)\n",
    "            shm_imgs_arr = np.ndarray(self.images.shape, dtype=self.images.dtype, buffer=shm_imgs.buf)\n",
    "            shm_imgs_arr[:] = self.images[:]\n",
    "            self.images = shm_imgs_arr # Do not keep dublicate \n",
    "            self.data_shm_imgs = ShmData(\n",
    "                shm_name=shm_imgs.name, shape=self.images.shape, \n",
    "                dtype=self.images.dtype\n",
    "            )\n",
    "            self._shm_imgs = shm_imgs\n",
    "            # Masks\n",
    "            shm_masks = shared_memory.SharedMemory(create=True, size=self.masks.nbytes)\n",
    "            shm_masks_arr = np.ndarray(self.masks.shape, dtype=self.masks.dtype, buffer=shm_masks.buf)\n",
    "            shm_masks_arr[:] = self.masks[:]\n",
    "            self.masks = shm_masks_arr # Do not keep dublicate \n",
    "            self.data_shm_masks = ShmData(\n",
    "                shm_name=shm_masks.name, shape=self.masks.shape,\n",
    "                dtype=self.masks.dtype\n",
    "            )\n",
    "            self._shm_masks = shm_masks\n",
    "            print(\"Shared memory are created for imgs and masks!\")\n",
    "\n",
    "    def close_shm(self):\n",
    "        if self.create_shared_memory:\n",
    "            # Make sure there is no reference data to images/masks\n",
    "            del self.images\n",
    "            del self.masks\n",
    "            self.images = []\n",
    "            self.masks = []\n",
    "            # Close and unlink\n",
    "            if self._shm_masks is not None:\n",
    "                self._shm_masks.close()\n",
    "                self._shm_masks.unlink()\n",
    "                self._shm_masks = None\n",
    "\n",
    "            if self._shm_imgs is not None:\n",
    "                self._shm_imgs.close()\n",
    "                self._shm_imgs.unlink()\n",
    "                self._shm_imgs = None\n",
    "            print(\"Shared memory for masks and images are success cleared!\")\n",
    "                    \n",
    "    \n",
    "    def _cut_with_window(self, image, mask, cut_window):\n",
    "        assert len(cut_window) == 2\n",
    "        h_win, w_win = cut_window\n",
    "        _, h, w = image.shape\n",
    "        h_parts = h // h_win\n",
    "        w_parts = w // w_win\n",
    "        if h % h_win != 0:\n",
    "            print(f\"{h % h_win} pixels will be dropped by h axis. Input shape={image.shape}\")\n",
    "\n",
    "        if w % w_win != 0:\n",
    "            print(f\"{w % w_win} pixels will be dropped by w axis. Input shape={image.shape}\")\n",
    "        return cut_into_parts(\n",
    "            image=image, mask=mask, h_parts=h_parts, w_parts=w_parts,\n",
    "            h_win=h_win, w_win=w_win\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5316ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pca_transformation(x, pca_mean, pca_components, pca_explained_variance):\n",
    "    if len(x.shape) == 3:\n",
    "        x_t = x.reshape((x.shape[0], -1)) # (C, H, W) -> (C, H * W)\n",
    "        x_t = np.transpose(x_t, (1, 0)) # (C, H * W) -> (H * W, C)\n",
    "        x_t = x_t - pca_mean\n",
    "        x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "        return x_t.reshape((x.shape[1], x.shape[2], pca_components.shape[0])).astype(np.float32, copy=False) # (H, W, N)\n",
    "    elif len(x.shape) == 4:\n",
    "        # x - (N, C, H, W)\n",
    "        x_t = np.transpose(x, (0, 2, 3, 1)) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x_t = x_t - pca_mean\n",
    "        x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "        x_t = np.transpose(x_t, (0, -1, 1, 2)) # (N, H, W, C) -> (N, C, H, W)\n",
    "        return x_t.astype(np.float32, copy=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown shape={x.shape}, must be of len 3 or 4.\")\n",
    "\n",
    "def standartization(img, mean, std):\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    return img\n",
    "\n",
    "def standartization_pool(mean, std):\n",
    "    # X shape - (N, C, H, W)\n",
    "    # from shape (comp,) -> (1, comp, 1, 1)\n",
    "    mean = np.expand_dims(np.expand_dims(np.array(mean, dtype=np.float32), axis=-1), axis=-1)\n",
    "    std = np.expand_dims(np.expand_dims(np.array(std, dtype=np.float32), axis=-1), axis=-1)\n",
    "    \n",
    "    return lambda x: standartization(x, mean=mean, std=std)\n",
    "\n",
    "\n",
    "def test_augmentation(image, mask, **kwargs):\n",
    "    image = torch.from_numpy(image)\n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    mask = torch.from_numpy(mask)\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 'kfold0'  # ['kfold0', 'kfold1', 'kfold2']\n",
    "cur_exp = ATT_EXP  #[ATT_EXP, WO_ATT_EXP, SW_ATT_EXP, SW_WO_ATT_EXP]\n",
    "run = 'R1' #['R1', 'R2', 'R3', 'R4']\n",
    "type_indices = 'test' # ['val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ab0f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_name = f'{cur_exp} {kfold} {run}'\n",
    "if kfold == 'kfold1':\n",
    "    num_classes = 7\n",
    "else:\n",
    "    num_classes = 8\n",
    "\n",
    "if cur_exp in [SW_ATT_EXP, SW_WO_ATT_EXP]:\n",
    "    PREFIX_INFO_PATH = f'/home/rustam/hyperspecter_segmentation/danil_cave/kfolds_data_with_other/swapped/{kfold}/data'\n",
    "else:\n",
    "    PREFIX_INFO_PATH = f'/home/rustam/hyperspecter_segmentation/danil_cave/kfolds_data_with_other/{kfold}/data'\n",
    "pca_explained_variance = np.load(f'{PREFIX_INFO_PATH}/{kfold}_PcaExplainedVariance_.npy')\n",
    "pca_mean = np.load(f'{PREFIX_INFO_PATH}/{kfold}_PcaMean.npy')\n",
    "pca_components = np.load(f'{PREFIX_INFO_PATH}/{kfold}_PcaComponents.npy')\n",
    "DATA_STANDARTIZATION_PARAMS_PATH = f'{PREFIX_INFO_PATH}/data_standartization_params_{kfold}.json'\n",
    "with open(DATA_STANDARTIZATION_PARAMS_PATH, 'r') as f:\n",
    "    DATA_STANDARTIZATION_PARAMS = json.load(f)\n",
    "\n",
    "pca_transformation_exp = lambda x: pca_transformation(x, pca_mean, pca_components, pca_explained_variance)\n",
    "\n",
    "def preprocessing(imgs, masks, map_mask_to_class=False, split_size=256): \n",
    "    mean, std = (\n",
    "        DATA_STANDARTIZATION_PARAMS.get('means'), \n",
    "        DATA_STANDARTIZATION_PARAMS.get('stds')\n",
    "    )\n",
    "    assert mean is not None and std is not None\n",
    "    print('Create np array of imgs and masks...')\n",
    "    imgs_np = np.asarray(imgs, dtype=np.float32) # (N, 237, 1, 1)\n",
    "    masks_np = np.asarray(masks, dtype=np.int64) # (N, 1, 1, 3)\n",
    "    print(\"Split imgs dataset...\")\n",
    "    imgs_split_np = np.array_split(imgs_np, split_size) # (split_size, Ns, 237, 1, 1)\n",
    "    print('Start preprocess images...')\n",
    "    # Wo PCA\n",
    "    # _images = [np.transpose(image, (1, 2, 0)) for image in imgs]\n",
    "    # W Pca\n",
    "    with Pool(18) as p:\n",
    "        _images = list(tqdm(p.imap(\n",
    "                pca_transformation_exp, \n",
    "                imgs_split_np,\n",
    "                #chunksize=1\n",
    "            ), total=len(imgs_split_np))\n",
    "        )\n",
    "        _images = list(tqdm(p.imap(\n",
    "            standartization_pool(mean=mean, std=std), \n",
    "            _images,\n",
    "            #chunksize=1\n",
    "            ), total=len(imgs_split_np))\n",
    "        )\n",
    "    _images = list(np.concatenate(_images, axis=0)) # (split_size, Ns, 237, 1, 1) -> (split_size * Ns, 237, 1, 1)\n",
    "    print(\"Preprocess masks...\")\n",
    "    _masks = list(np.transpose(masks_np[..., 0:1], (0, -1, 1, 2)))\n",
    "    print(\"Finish preprocess!\")\n",
    "    return _images, _masks\n",
    "\n",
    "test_indices = np.load(f'{PREFIX_INFO_PATH}/{kfold}_indx_unknown_test.npy')\n",
    "val_indices = np.load(f'{PREFIX_INFO_PATH}/{kfold}_indx_test.npy')\n",
    "train_indices = np.load(f'{PREFIX_INFO_PATH}/{kfold}_indx_train.npy')\n",
    "path_old2new = f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    "\n",
    "old2new_mapper = DatasetRemapper(np.load(path_old2new))\n",
    "\n",
    "if cur_exp in [SW_ATT_EXP, SW_WO_ATT_EXP]:\n",
    "    w_sorted = sorted(glob.glob(f'swapped_other/pytorch_li_logs/{exp_name2id.get(net_name)}/*'), \n",
    "           key=lambda x: -float(x.split('/')[-1].split('-')[-1][9:13])\n",
    "    )\n",
    "else:\n",
    "    w_sorted = sorted(glob.glob(f'original_other/pytorch_li_logs/{exp_name2id.get(net_name)}/*'), \n",
    "           key=lambda x: -float(x.split('/')[-1].split('-')[-1][9:13])\n",
    "    )\n",
    "\n",
    "\n",
    "pick_best_one = w_sorted[0]\n",
    "if cur_exp in [ATT_EXP, SW_ATT_EXP]:\n",
    "    net = MySuperAttentionNetLittleInput(INPUT_DIM, num_classes) \n",
    "else:\n",
    "    net = MySuperNetLittleInput(INPUT_DIM, num_classes) \n",
    "\n",
    "model = NnModel.load_from_checkpoint(\n",
    "    pick_best_one,\n",
    "    loss=nn.CrossEntropyLoss(), model=net\n",
    ")\n",
    "net = model.model\n",
    "net.eval()\n",
    "net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc10476",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_indices == 'val':\n",
    "    indices = val_indices\n",
    "else:\n",
    "    indices = test_indices\n",
    "\n",
    "dataset_creator_val = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=indices, cut_window=DATA_SHAPE,\n",
    "    create_shared_memory=False\n",
    ")\n",
    "dataset_val = HsiDataloaderCutter(\n",
    "    images=dataset_creator_val.images, masks=dataset_creator_val.masks,\n",
    "    augmentation=test_augmentation,\n",
    "    shuffle_data=False, cut_window=DATA_SHAPE,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a08ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_s, mask_s = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edde2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, remapped_mask_np = old2new_mapper(None, mask_s.numpy())\n",
    "img_s = img_s.to(device=device)\n",
    "pred = net(img_s)\n",
    "pred = nn.functional.softmax(pred, dim=1)\n",
    "pred = pred.cpu().detach().numpy()\n",
    "pred = np.transpose(pred, [0, 2, 3, 1])\n",
    "pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "mask_s = torch.squeeze(mask_s).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f86356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_pac_cam(net, x):\n",
    "    x = net.backbone(x)\n",
    "    _, attention_pam = net.pam_module(x, True)\n",
    "    _, attention_cam = net.cam_module(x, True)\n",
    "    return attention_pam, attention_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_result_pac_cam(net, x):\n",
    "    x = net.backbone(x)\n",
    "    attention_pam_result = net.pam_module(x)\n",
    "    attention_cam_result = net.cam_module(x)\n",
    "    return attention_pam_result, attention_cam_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1740af",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data_x, val_data = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cddaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    data = in_data_x.to(device=device)\n",
    "    preds = net(data).cpu().detach() # (divided_batch, num_classes)\n",
    "    preds_attention_pam, preds_attention_cam  = get_attention_pac_cam(net, data)\n",
    "    preds_attention_pam_result, preds_attention_cam_result  = get_attention_result_pac_cam(net, data)\n",
    "    preds_attention_pam = preds_attention_pam.cpu().detach()\n",
    "    preds_attention_cam = preds_attention_cam.cpu().detach()\n",
    "    preds_attention_pam_result = preds_attention_pam_result.cpu().detach()\n",
    "    preds_attention_cam_result = preds_attention_cam_result.cpu().detach()\n",
    "\n",
    "pred_attention_pam_view = preds_attention_pam.view(1, 32, 32, 32, 32)\n",
    "pred_attention_cam_view = preds_attention_cam.view(1, 128, 128)\n",
    "pred_attention_pam_view_result = preds_attention_pam_result.view(1, 128, 32, 32)\n",
    "pred_attention_cam_view_result = preds_attention_cam_result.view(1, 128, 32, 32)\n",
    "target = val_data.clone()\n",
    "in_data_x_tensor = in_data_x.clone()\n",
    "pred_mask = torch.argmax(torch.nn.functional.softmax(preds, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "sns.heatmap(pred_mask[0], vmin=0, vmax=NUM_CLASSES)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "sns.heatmap(target[0], vmin=0, vmax=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663eecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_argmax = torch.argmax(torch.nn.functional.softmax(preds[indx_b], dim=0), dim=0)\n",
    "sns.heatmap(pred_argmax, vmin=0, vmax=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(target[indx_b], vmin=0, vmax=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06edefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 1, 1\n",
    "plt.plot(x,y,'bo') \n",
    "sns.heatmap(pred_attention_pam_view[indx_b][x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = 128\n",
    "att_size = 32\n",
    "\n",
    "\n",
    "attention_pam = torch.clone(pred_attention_pam_view[indx_b])\n",
    "\n",
    "count_x = 4\n",
    "count_y = 4\n",
    "\n",
    "size_x = att_size // count_x\n",
    "size_y = att_size // count_y\n",
    "y_i, x_i = 0, 0\n",
    "mid_x, mid_y = size_x // 2, size_y // 2\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i in range(1, count_x * count_y + 1):\n",
    "    fig.add_subplot(count_x, count_y, i)\n",
    "    if x_i == count_x:\n",
    "        x_i = 0\n",
    "        y_i += 1\n",
    "    x, y = mid_x + x_i * size_x, mid_y + y_i * size_y\n",
    "    att_s = attention_pam[y, x].unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    att_s = torch.nn.functional.interpolate(att_s, (128, 128)).squeeze()\n",
    "    sns.heatmap(att_s)\n",
    "    \n",
    "    x *= (orig_size // att_size)\n",
    "    y *= (orig_size // att_size)\n",
    "    plt.plot(x,y,'bo') \n",
    "    \n",
    "    x_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be4d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_show = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(3, 2, 1,)\n",
    "ax = sns.heatmap((-1)*pred_attention_cam_view_result[indx_show, 0])\n",
    "ax.set(xlabel='cam')\n",
    "\n",
    "fig.add_subplot(3, 2, 2)\n",
    "ax = sns.heatmap((-1)*pred_attention_pam_view_result[indx_show, 0])\n",
    "ax.set(xlabel='pam')\n",
    "\n",
    "fig.add_subplot(3, 2, 3)\n",
    "ax = sns.heatmap(in_data_x_tensor[indx_show, 0])\n",
    "ax.set(xlabel='input')\n",
    "\n",
    "fig.add_subplot(3, 2, 4)\n",
    "ax = sns.heatmap(target[indx_show], vmin=0, vmax=NUM_CLASSES)\n",
    "ax.set(xlabel='mask')\n",
    "\n",
    "fig.add_subplot(3, 2, 5)\n",
    "pred_argmax = torch.argmax(torch.nn.functional.softmax(preds[indx_show], dim=0), dim=0)\n",
    "ax = sns.heatmap(pred_argmax, vmin=0, vmax=NUM_CLASSES)\n",
    "_ = ax.set(xlabel='pred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ed4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(3, 2, 1,)\n",
    "ax = sns.heatmap(pred_attention_cam_view_result[indx_show].mean(axis=0))\n",
    "ax.set(xlabel='cam')\n",
    "\n",
    "fig.add_subplot(3, 2, 2)\n",
    "ax = sns.heatmap(pred_attention_pam_view_result[indx_show].mean(axis=0))\n",
    "ax.set(xlabel='pam')\n",
    "\n",
    "fig.add_subplot(3, 2, 3)\n",
    "ax = sns.heatmap(in_data_x_tensor[indx_show, 0])\n",
    "ax.set(xlabel='input')\n",
    "\n",
    "fig.add_subplot(3, 2, 4)\n",
    "ax = sns.heatmap(target[indx_show], vmin=0, vmax=NUM_CLASSES)\n",
    "ax.set(xlabel='mask')\n",
    "\n",
    "fig.add_subplot(3, 2, 5)\n",
    "pred_argmax = torch.argmax(torch.nn.functional.softmax(preds[indx_show], dim=0), dim=0)\n",
    "ax = sns.heatmap(pred_argmax, vmin=0, vmax=NUM_CLASSES)\n",
    "_ = ax.set(xlabel='pred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d8dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(in_data_x_tensor[indx_b][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ff285",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i in range(1, 17):\n",
    "    fig.add_subplot(count_x, count_y, i)\n",
    "    sns.heatmap(in_data_x_tensor[indx_b][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd305ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data_x = in_data_x_tensor[indx_b]\n",
    "in_data_x_tensor_normed = (s_data_x - s_data_x.min()) / (s_data_x.max() - s_data_x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(in_data_x_tensor_normed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i in range(1, 17):\n",
    "    fig.add_subplot(count_x, count_y, i)\n",
    "    sns.heatmap(in_data_x_tensor_normed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f47063",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(torch.sqrt(torch.sum(torch.square(in_data_x_tensor[indx_b]), dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(torch.sum(in_data_x_tensor[indx_b], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c177e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'bo') \n",
    "sns.heatmap(pred_attention_cam[indx_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b3a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218dd75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a48e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56381c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(\n",
    "    np.asarray(target_list).reshape(-1),\n",
    "    np.asarray(preds_list).reshape(-1), \n",
    "    average='macro'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(\n",
    "    np.asarray(target_list).reshape(-1),\n",
    "    np.asarray(preds_list).reshape(-1), \n",
    "    average='weighted'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104ce71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
