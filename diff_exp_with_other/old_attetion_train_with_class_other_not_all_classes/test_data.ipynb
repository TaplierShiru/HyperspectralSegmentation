{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GPU_ID = \"3\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/makitorch')\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/')\n",
    "\n",
    "PREFIX_INFO_PATH = '/home/rustam/hyperspecter_segmentation/danil_cave/kfolds_data_with_other/kfold0/data'\n",
    "PATH_DATA = '/raid/rustam/hyperspectral_dataset/new_cropped_hsi_data'\n",
    "NUM_CLASSES = 8\n",
    "\n",
    "import random\n",
    "from multiprocessing.dummy import Pool\n",
    "from multiprocessing import shared_memory\n",
    "import copy\n",
    "from makitorch import *\n",
    "from makitorch.dataset_remapper import DatasetRemapper\n",
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import comet_ml\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "from Losses import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from makitorch.architectures.U2Net import U2Net\n",
    "\n",
    "from hsi_dataset_api import HsiDataset\n",
    "\n",
    "from makitorch.dataloaders.HsiDataloader import HsiDataloader\n",
    "from makitorch.architectures.Unet import Unet, UnetWithFeatureSelection\n",
    "from makitorch.loss import muti_bce_loss_fusion\n",
    "from sklearn.metrics import jaccard_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "from makitorch.data_tools.augmentation import DataAugmentator\n",
    "from makitorch.data_tools.augmentation import BaseDataAugmentor\n",
    "from makitorch.data_tools.preprocessing import BaseDataPreprocessor\n",
    "from makitorch.data_tools.preprocessing import DataPreprocessor\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from hsi_dataset_api import HsiDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def cut_into_parts(\n",
    "        image: np.ndarray, mask: np.ndarray, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    image_parts_list = []\n",
    "    mask_parts_list = []\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            img_part = image[:, \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "            mask_part = mask[\n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "\n",
    "            image_parts_list.append(img_part)\n",
    "            mask_parts_list.append(mask_part)\n",
    "    return image_parts_list, mask_parts_list\n",
    "\n",
    "\n",
    "class ShmData:\n",
    "\n",
    "    def __init__(self, shm_name, shape, dtype):\n",
    "        self.shm_name = shm_name\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "class DatasetCreator:\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_path: str,\n",
    "            preprocessing: Optional[Union[DataPreprocessor, Callable]] = BaseDataPreprocessor(),\n",
    "            indices = None,\n",
    "            cut_window=(8, 8),\n",
    "            map_mask_to_class=False,\n",
    "            create_shared_memory=False,\n",
    "            shuffle_then_prepared=False,\n",
    "            path_old2new: str = None):\n",
    "        self.dataset = HsiDataset(data_path)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.cut_window = cut_window\n",
    "        self.map_mask_to_class = map_mask_to_class\n",
    "        self.create_shared_memory = create_shared_memory\n",
    "        \n",
    "        if path_old2new:\n",
    "            self.old2new_mapper = DatasetRemapper(np.load(path_old2new))\n",
    "        else:\n",
    "            self.old2new_mapper = None\n",
    "\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        self._shm_imgs = None\n",
    "        self._shm_masks = None\n",
    "\n",
    "        for idx, data_point in tqdm(enumerate(self.dataset.data_iterator(opened=True, shuffle=False))):\n",
    "            if indices is not None and idx not in indices:\n",
    "                continue\n",
    "            image, mask = data_point.hsi, data_point.mask\n",
    "\n",
    "            if self.old2new_mapper:\n",
    "                _, mask = self.old2new_mapper(None, mask)\n",
    "\n",
    "            if cut_window is not None:\n",
    "                image_parts, mask_parts = self._cut_with_window(image, mask, cut_window)\n",
    "                self.images += image_parts\n",
    "                self.masks += mask_parts\n",
    "            else:\n",
    "                self.images.append(image)\n",
    "                self.masks.append(mask)\n",
    "        print(\"Preprocess data...\")\n",
    "        if self.preprocessing is not None:\n",
    "            self.images, self.masks = self.preprocessing(\n",
    "                self.images, self.masks, map_mask_to_class=map_mask_to_class\n",
    "            )\n",
    "\n",
    "        if shuffle_then_prepared:\n",
    "            self.images, self.masks = shuffle(self.images, self.masks)\n",
    "\n",
    "        # Create shared memory\n",
    "        if create_shared_memory:\n",
    "            print('Create shared memory...')\n",
    "            # First - map images and masks into np\n",
    "            self.images = np.asarray(self.images, dtype=np.float32)\n",
    "            self.masks = np.asarray(self.masks, dtype=np.int64)\n",
    "            # Imgs\n",
    "            shm_imgs = shared_memory.SharedMemory(create=True, size=self.images.nbytes)\n",
    "            shm_imgs_arr = np.ndarray(self.images.shape, dtype=self.images.dtype, buffer=shm_imgs.buf)\n",
    "            shm_imgs_arr[:] = self.images[:]\n",
    "            self.images = shm_imgs_arr # Do not keep dublicate \n",
    "            self.data_shm_imgs = ShmData(\n",
    "                shm_name=shm_imgs.name, shape=self.images.shape, \n",
    "                dtype=self.images.dtype\n",
    "            )\n",
    "            self._shm_imgs = shm_imgs\n",
    "            # Masks\n",
    "            shm_masks = shared_memory.SharedMemory(create=True, size=self.masks.nbytes)\n",
    "            shm_masks_arr = np.ndarray(self.masks.shape, dtype=self.masks.dtype, buffer=shm_masks.buf)\n",
    "            shm_masks_arr[:] = self.masks[:]\n",
    "            self.masks = shm_masks_arr # Do not keep dublicate \n",
    "            self.data_shm_masks = ShmData(\n",
    "                shm_name=shm_masks.name, shape=self.masks.shape,\n",
    "                dtype=self.masks.dtype\n",
    "            )\n",
    "            self._shm_masks = shm_masks\n",
    "            print(\"Shared memory are created for imgs and masks!\")\n",
    "\n",
    "    def close_shm(self):\n",
    "        if self.create_shared_memory:\n",
    "            # Make sure there is no reference data to images/masks\n",
    "            del self.images\n",
    "            del self.masks\n",
    "            self.images = []\n",
    "            self.masks = []\n",
    "            # Close and unlink\n",
    "            if self._shm_masks is not None:\n",
    "                self._shm_masks.close()\n",
    "                self._shm_masks.unlink()\n",
    "                self._shm_masks = None\n",
    "\n",
    "            if self._shm_imgs is not None:\n",
    "                self._shm_imgs.close()\n",
    "                self._shm_imgs.unlink()\n",
    "                self._shm_imgs = None\n",
    "            print(\"Shared memory for masks and images are success cleared!\")\n",
    "                    \n",
    "    \n",
    "    def _cut_with_window(self, image, mask, cut_window):\n",
    "        assert len(cut_window) == 2\n",
    "        h_win, w_win = cut_window\n",
    "        _, h, w = image.shape\n",
    "        h_parts = h // h_win\n",
    "        w_parts = w // w_win\n",
    "        if h % h_win != 0:\n",
    "            print(f\"{h % h_win} pixels will be dropped by h axis. Input shape={image.shape}\")\n",
    "\n",
    "        if w % w_win != 0:\n",
    "            print(f\"{w % w_win} pixels will be dropped by w axis. Input shape={image.shape}\")\n",
    "        return cut_into_parts(\n",
    "            image=image, mask=mask, h_parts=h_parts, w_parts=w_parts,\n",
    "            h_win=h_win, w_win=w_win\n",
    "        )\n",
    "\n",
    "\n",
    "# Define a `worker_init_fn` that configures each dataset copy differently\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset = worker_info.dataset\n",
    "    shared_memory_imgs_data: ShmData = dataset.shared_memory_imgs_data\n",
    "    shared_memory_masks_data: ShmData = dataset.shared_memory_masks_data\n",
    "    if shared_memory_imgs_data is not None and shared_memory_masks_data is not None:\n",
    "        # Take array from memory\n",
    "        existing_shm_imgs = shared_memory.SharedMemory(name=shared_memory_imgs_data.shm_name)\n",
    "        dataset_imgs_np = np.ndarray(\n",
    "            shared_memory_imgs_data.shape, \n",
    "            dtype=shared_memory_imgs_data.dtype, buffer=existing_shm_imgs.buf\n",
    "        )\n",
    "        dataset.shm_imgs = existing_shm_imgs\n",
    "        existing_shm_masks = shared_memory.SharedMemory(name=shared_memory_masks_data.shm_name)\n",
    "        dataset_masks_np = np.ndarray(\n",
    "            shared_memory_masks_data.shape, \n",
    "            dtype=shared_memory_masks_data.dtype, buffer=existing_shm_masks.buf\n",
    "        )\n",
    "        dataset.shm_masks = existing_shm_masks\n",
    "    else:\n",
    "        assert dataset.images is not None and dataset.masks is not None\n",
    "        dataset_imgs_np = dataset.images\n",
    "        dataset_masks_np = dataset.masks\n",
    "    overall_start = 0\n",
    "    overall_end = len(dataset_imgs_np)\n",
    "    # configure the dataset to only process the split workload\n",
    "    per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "    worker_id = worker_info.id\n",
    "    start = overall_start + worker_id * per_worker\n",
    "    end = min(start + per_worker, overall_end)\n",
    "    dataset.images = list(dataset_imgs_np[start:end])\n",
    "    dataset.masks = list(dataset_masks_np[start:end])\n",
    "\n",
    "\n",
    "class HsiDataloaderCutter(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images, masks,\n",
    "            augmentation: Optional[Union[DataAugmentator, Callable]] = BaseDataAugmentor(),\n",
    "            shuffle_data=False,\n",
    "            cut_window=(8, 8),\n",
    "            map_mask_to_class=False,\n",
    "            shared_memory_imgs_data: ShmData = None,\n",
    "            shared_memory_masks_data: ShmData = None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.shuffle_data = shuffle_data\n",
    "        self.augmentation = augmentation\n",
    "        if cut_window is not None and cut_window[0] == 1 and cut_window[1] == 1:\n",
    "            self.ignore_image_augs = True\n",
    "        else:\n",
    "            self.ignore_image_augs = False\n",
    "        self.cut_window = cut_window\n",
    "        self.map_mask_to_class = map_mask_to_class\n",
    "        self.shared_memory_imgs_data = shared_memory_imgs_data\n",
    "        self.shared_memory_masks_data = shared_memory_masks_data\n",
    "        \n",
    "        self.shm_imgs: shared_memory.SharedMemory = None\n",
    "        self.shm_masks: shared_memory.SharedMemory = None\n",
    "\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert self.images is not None and self.masks is not None\n",
    "        if self.shuffle_data:\n",
    "            self.images, self.masks = shuffle(self.images, self.masks)\n",
    "        \n",
    "        for image, mask in zip(self.images, self.masks):\n",
    "            yield self.augmentation(\n",
    "                image, mask, \n",
    "                map_mask_to_class=self.map_mask_to_class,\n",
    "                ignore_image_augs=self.ignore_image_augs\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "device = 'cuda:0'\n",
    "pca_explained_variance = np.load(f'{PREFIX_INFO_PATH}/kfold0_PcaExplainedVariance_.npy')\n",
    "pca_mean = np.load(f'{PREFIX_INFO_PATH}/kfold0_PcaMean.npy')\n",
    "pca_components = np.load(f'{PREFIX_INFO_PATH}/kfold0_PcaComponents.npy')\n",
    "\n",
    "\n",
    "def pca_transformation(x):\n",
    "    if len(x.shape) == 3:\n",
    "        x_t = x.reshape((x.shape[0], -1)) # (C, H, W) -> (C, H * W)\n",
    "        x_t = np.transpose(x_t, (1, 0)) # (C, H * W) -> (H * W, C)\n",
    "        x_t = x_t - pca_mean\n",
    "        x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "        return x_t.reshape((x.shape[1], x.shape[2], pca_components.shape[0])).astype(np.float32, copy=False) # (H, W, N)\n",
    "    elif len(x.shape) == 4:\n",
    "        # x - (N, C, H, W)\n",
    "        x_t = np.transpose(x, (0, 2, 3, 1)) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x_t = x_t - pca_mean\n",
    "        x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "        x_t = np.transpose(x_t, (0, -1, 1, 2)) # (N, H, W, C) -> (N, C, H, W)\n",
    "        return x_t.astype(np.float32, copy=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown shape={x.shape}, must be of len 3 or 4.\")\n",
    "\n",
    "def standartization(img, mean, std):\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    return img\n",
    "\n",
    "def standartization_pool(mean, std):\n",
    "    # X shape - (N, C, H, W)\n",
    "    # from shape (comp,) -> (1, comp, 1, 1)\n",
    "    mean = np.expand_dims(np.expand_dims(np.array(mean, dtype=np.float32), axis=-1), axis=-1)\n",
    "    std = np.expand_dims(np.expand_dims(np.array(std, dtype=np.float32), axis=-1), axis=-1)\n",
    "    \n",
    "    return lambda x: standartization(x, mean=mean, std=std)\n",
    "\n",
    "\n",
    "def mask2class(mask):\n",
    "    # Calculate which class have more pixel count\n",
    "    max_value = -1\n",
    "    pixel_count = -1\n",
    "    for class_indx in np.unique(mask):\n",
    "        pix_count_s = np.sum(mask == class_indx)\n",
    "        if pix_count_s > pixel_count:\n",
    "            max_value = class_indx\n",
    "            pixel_count = pix_count_s\n",
    "    assert max_value != -1\n",
    "    return np.array([max_value], dtype=np.int64) \n",
    "\n",
    "\n",
    "def preprocessing(imgs, masks, map_mask_to_class=False, split_size=256):\n",
    "    with open(f'{PREFIX_INFO_PATH}/data_standartization_params_kfold0.json', 'r') as f:\n",
    "        data_standartization_params = json.load(f)\n",
    "    mean = data_standartization_params.get('means')\n",
    "    std = data_standartization_params.get('stds')\n",
    "    assert mean is not None and std is not None\n",
    "    print('Create np array of imgs and masks...')\n",
    "    imgs_np = np.asarray(imgs, dtype=np.float32) # (N, 237, 1, 1)\n",
    "    masks_np = np.asarray(masks, dtype=np.int64) # (N, 1, 1, 3)\n",
    "    print(\"Split imgs dataset...\")\n",
    "    imgs_split_np = np.array_split(imgs_np, split_size) # (split_size, Ns, 237, 1, 1)\n",
    "    print('Start preprocess images...')\n",
    "    # Wo PCA\n",
    "    # _images = [np.transpose(image, (1, 2, 0)) for image in imgs]\n",
    "    # W Pca\n",
    "    with Pool(18) as p:\n",
    "        _images = list(tqdm(p.imap(\n",
    "                pca_transformation, \n",
    "                imgs_split_np,\n",
    "                #chunksize=1\n",
    "            ), total=len(imgs_split_np))\n",
    "        )\n",
    "        _images = list(tqdm(p.imap(\n",
    "            standartization_pool(mean=mean, std=std), \n",
    "            _images,\n",
    "            #chunksize=1\n",
    "            ), total=len(imgs_split_np))\n",
    "        )\n",
    "    _images = list(np.concatenate(_images, axis=0)) # (split_size, Ns, 237, 1, 1) -> (split_size * Ns, 237, 1, 1)\n",
    "    print(\"Preprocess masks...\")\n",
    "    _masks = list(np.transpose(masks_np[..., 0:1], (0, -1, 1, 2)))\n",
    "    print(\"Finish preprocess!\")\n",
    "    if map_mask_to_class:\n",
    "        _masks = [mask2class(mask) for mask in _masks]\n",
    "    return _images, _masks\n",
    "\n",
    "\n",
    "def test_augmentation(image, mask, **kwargs):\n",
    "    image = torch.from_numpy(image)\n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    mask = torch.from_numpy(mask)\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "\n",
    "def aug_random_rotate(image, mask, map_mask_to_class=False, **kwargs):\n",
    "    angle = T.RandomRotation.get_params((-30, 30))\n",
    "    image = TF.rotate(image, angle, interpolation=T.InterpolationMode.BILINEAR)\n",
    "    if not map_mask_to_class:\n",
    "        mask = TF.rotate(mask, angle, interpolation=T.InterpolationMode.NEAREST)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def aug_flip_horizontal(image, mask, map_mask_to_class=False, **kwargs):\n",
    "    if torch.rand(1) > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "        if not map_mask_to_class:\n",
    "            mask = TF.hflip(mask)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def aug_flip_vertical(image, mask, map_mask_to_class=False, **kwargs):\n",
    "    if torch.rand(1) > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "        if not map_mask_to_class:\n",
    "            mask = TF.vflip(mask)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "MASK_AUG_SCALE = 100\n",
    "MASK_AUG_COMPARE = 90\n",
    "RandomEraseTorch = T.RandomErasing(\n",
    "    p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), \n",
    "    value='random', inplace=False\n",
    ")\n",
    "def aug_random_erase(image, mask, map_mask_to_class=False, **kwargs):\n",
    "    # Create mask in order to take area of aug\n",
    "    mask_aug_area = torch.ones(\n",
    "        1, image.shape[1], image.shape[2], \n",
    "        dtype=image.dtype, device=image.device\n",
    "    ) * MASK_AUG_SCALE\n",
    "    in_x = torch.cat([image, mask_aug_area], dim=0)\n",
    "    # Apply aug\n",
    "    img_aug = RandomEraseTorch(in_x)\n",
    "    image = img_aug[:-1]\n",
    "    if not map_mask_to_class:\n",
    "        mask_aug = img_aug[-1:]\n",
    "        # Take mask and reverse values\n",
    "        # 0 - cutout zone, 1 - good zone\n",
    "        mask_aug = (mask_aug > MASK_AUG_COMPARE).long()\n",
    "        mask = mask * mask_aug\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "AUGS_LIST = [\n",
    "    aug_random_rotate,\n",
    "    aug_flip_horizontal,\n",
    "    aug_flip_vertical,\n",
    "    aug_random_erase\n",
    "]\n",
    "\n",
    "\n",
    "def augmentation(image, mask, map_mask_to_class=False, ignore_image_augs=False):\n",
    "    image = torch.from_numpy(image)\n",
    "    mask = torch.from_numpy(mask)\n",
    "    if not ignore_image_augs:\n",
    "        for aug_func in AUGS_LIST:\n",
    "            image, mask = aug_func(image, mask, map_mask_to_class=map_mask_to_class)\n",
    "    \n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "USE_SHM = True\n",
    "\n",
    "if USE_SHM:\n",
    "    NUM_WORKERS = 10\n",
    "else:\n",
    "    NUM_WORKERS = 5\n",
    "\n",
    "N_REPEAT_EXP = 2\n",
    "\n",
    "T_0_param = 2\n",
    "T_mult_param = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_window = (128, 128)\n",
    "batch_size = 1\n",
    "test_indices = np.load(f'{PREFIX_INFO_PATH}/kfold0_indx_unknown_test.npy')\n",
    "val_indices = np.load(f'{PREFIX_INFO_PATH}/kfold0_indx_test.npy')\n",
    "train_indices = np.load(f'{PREFIX_INFO_PATH}/kfold0_indx_train.npy')\n",
    "\n",
    "# Load and preprocess data\n",
    "# Train\n",
    "if USE_SHM:\n",
    "    print(\"Using shared memory...\")\n",
    "    dataset_creator_train = DatasetCreator(\n",
    "        PATH_DATA, preprocessing=preprocessing, \n",
    "        indices=train_indices, cut_window=cut_window,\n",
    "        create_shared_memory=True, shuffle_then_prepared=True,\n",
    "        path_old2new=f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    "    )\n",
    "    dataset_train = HsiDataloaderCutter(\n",
    "        images=None, masks=None,\n",
    "        augmentation=augmentation,\n",
    "        shuffle_data=True, cut_window=cut_window,\n",
    "        shared_memory_imgs_data=dataset_creator_train.data_shm_imgs,\n",
    "        shared_memory_masks_data=dataset_creator_train.data_shm_masks,\n",
    "    )\n",
    "else:\n",
    "    dataset_creator_train = DatasetCreator(\n",
    "        PATH_DATA, preprocessing=preprocessing, \n",
    "        indices=train_indices, cut_window=cut_window,\n",
    "        create_shared_memory=False, shuffle_then_prepared=True,\n",
    "        path_old2new=f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    "    )\n",
    "    dataset_train = HsiDataloaderCutter(\n",
    "        images=dataset_creator_train.images, masks=dataset_creator_train.masks,\n",
    "        augmentation=augmentation,\n",
    "        shuffle_data=True, cut_window=cut_window,\n",
    "    )\n",
    "print(f\"Number of workers={NUM_WORKERS}\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, \n",
    "    num_workers=NUM_WORKERS, pin_memory=False, prefetch_factor=2,\n",
    "    worker_init_fn=worker_init_fn, drop_last=True\n",
    ")\n",
    "# Validation\n",
    "dataset_creator_val = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=val_indices, cut_window=None,\n",
    "    create_shared_memory=False,\n",
    "    path_old2new=f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    ")\n",
    "dataset_val = HsiDataloaderCutter(\n",
    "    images=dataset_creator_val.images, masks=dataset_creator_val.masks,\n",
    "    augmentation=test_augmentation,\n",
    "    shuffle_data=False, cut_window=None,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1)\n",
    "\n",
    "# Test\n",
    "dataset_creator_test = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=test_indices, cut_window=None,\n",
    "    create_shared_memory=False,\n",
    "    path_old2new=f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    ")\n",
    "dataset_test = HsiDataloaderCutter(\n",
    "    images=dataset_creator_test.images, masks=dataset_creator_test.masks,\n",
    "    augmentation=test_augmentation,\n",
    "    shuffle_data=False, cut_window=None,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b914547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_index = test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a170e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_index = test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfaaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "dataset_creator_test = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=part_index, cut_window=None,\n",
    "    create_shared_memory=False,\n",
    "    path_old2new=f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    ")\n",
    "dataset_test = HsiDataloaderCutter(\n",
    "    images=dataset_creator_test.images, masks=dataset_creator_test.masks,\n",
    "    augmentation=test_augmentation,\n",
    "    shuffle_data=False, cut_window=None,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_mask = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90bfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(test_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(test_image[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3ddfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_creator_test = DatasetCreator(\n",
    "    PATH_DATA, preprocessing=preprocessing, \n",
    "    indices=np.asarray(range(362)), cut_window=None,\n",
    "    create_shared_memory=False,\n",
    "    path_old2new=f'{PREFIX_INFO_PATH}/index2class.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccc6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images=dataset_creator_test.images; masks=dataset_creator_test.masks\n",
    "N = len(images)\n",
    "counter = 0\n",
    "size = 5\n",
    "part_s = (N // (size * size)) + (N % (size * size) != 0)\n",
    "for p_i in range(part_s):\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    for i in range(1, min(size * size + 1, N - p_i * size * size)):\n",
    "        img = masks[counter][0]\n",
    "        fig.add_subplot(size, size, i)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{counter}\")\n",
    "        sns.heatmap(img, vmin=0, vmax=8)\n",
    "        counter += 1\n",
    "    plt.savefig(f\"res_{p_i}\", dpi=350)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ac9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
