{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cae7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_f = torch.randn(1, 17, 128, 128).float()\n",
    "\n",
    "global_f = torch.randn(1, 17, 128, 128).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977de985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_f, normalize_attn=True):\n",
    "        super(LinearAttentionBlock, self).__init__()\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.conv_pointwise = nn.Conv2d(\n",
    "            in_channels=in_f, out_channels=1, \n",
    "            kernel_size=1, padding=0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, low_f, global_f):\n",
    "        N, C, W, H = low_f.size()\n",
    "        # (Batch size, 1, H, W)\n",
    "        x_pointwised = self.conv_pointwise(low_f + global_f)\n",
    "        if self.normalize_attn:\n",
    "            # (Batch size, 1, H, W) -> (Batch size, 1, H * W)\n",
    "            x_flatten = x_pointwised.view(N, 1, -1)\n",
    "            # (Batch size, 1, H, W)\n",
    "            x_attention = F.softmax(x_flatten, dim=2).view(N, 1, H, W)\n",
    "        else:\n",
    "            x_attention = torch.sigmoid(x_pointwised)\n",
    "        # Apply attention to our input local features\n",
    "        f_attented = torch.mul(x_pointwised.expand_as(low_f), low_f)\n",
    "        if self.normalize_attn:\n",
    "            # (Batch size, C, H, W) -> (Batch size, C, H * W) -> (Batch size, C)\n",
    "            f_attented = f_attented.view(N, C, -1).sum(dim=2)\n",
    "        else:\n",
    "            f_attented = F.adaptive_avg_pool2d(f_attented, (1, 1)).view(N, C)\n",
    "        x_pointwised = x_pointwised.view(N, 1, H, W)\n",
    "        return x_pointwised, f_attented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_l = LinearAttentionBlock(17, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d41788",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pointwised, f_attented = att_l(low_f, global_f)\n",
    "x_pointwised.shape, f_attented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b37afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.imshow(x_pointwised[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b37ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_attented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc777802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAM_Module(nn.Module):\n",
    "    \"\"\" \n",
    "    Position attention module\n",
    "\n",
    "    \"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim, dim_reduse: int = 8):\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim // dim_reduse, \n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim // dim_reduse,\n",
    "             kernel_size=1\n",
    "        )\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, \n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attention\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CAM_Module(nn.Module):\n",
    "    \"\"\" \n",
    "    Channel attention module\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "       \n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attention\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe02775",
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_l = PAM_Module(17)\n",
    "cam_l = CAM_Module(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pam = pam_l(global_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cam = cam_l(global_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954870ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c099fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
