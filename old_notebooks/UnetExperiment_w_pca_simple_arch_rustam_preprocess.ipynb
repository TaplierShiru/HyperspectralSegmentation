{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12720b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/makitorch')\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60264d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makitorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import comet_ml\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "from Losses import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from makitorch.architectures.U2Net import U2Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsi_dataset_api import HsiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makitorch.dataloaders.HsiDataloader import HsiDataloader\n",
    "from makitorch.architectures.Unet import Unet, UnetWithFeatureSelection\n",
    "from makitorch.loss import muti_bce_loss_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "np.set_printoptions(suppress=True)\n",
    "def matrix2onehot(matrix, num_classes=17):\n",
    "    matrix = matrix.copy().reshape(-1)\n",
    "    one_hoted = np.zeros((matrix.size, num_classes))\n",
    "    one_hoted[np.arange(matrix.size),matrix] = 1\n",
    "    return one_hoted\n",
    "\n",
    "\n",
    "def calculate_iou(eval_loader, model, num_classes=17, loss=None):\n",
    "    res_list = []\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    loss_list = []\n",
    "    \n",
    "    for in_data_x, val_data in iter(eval_loader):\n",
    "        \n",
    "        preds = model(in_data_x)\n",
    "        if loss is not None:\n",
    "            loss_list.append(\n",
    "                loss(preds, val_data).cpu().detach().numpy()\n",
    "            )\n",
    "        else:\n",
    "            loss_list.append(None)\n",
    "        \n",
    "        preds = nn.functional.softmax(preds, dim=1).cpu().detach().numpy()\n",
    "        preds = np.squeeze(np.argmax(preds, axis=1))\n",
    "        target = np.squeeze(val_data.cpu().detach().numpy())\n",
    "        \n",
    "        target_list.append(target)\n",
    "        pred_list.append(preds)\n",
    "        \n",
    "        preds_one_hoted = matrix2onehot(preds, num_classes=num_classes)\n",
    "        target_one_hoted = matrix2onehot(target, num_classes=num_classes)\n",
    "        res = jaccard_score(target_one_hoted, preds_one_hoted, average=None, zero_division=1)\n",
    "        res_list.append(\n",
    "            res\n",
    "        )\n",
    "    \n",
    "    res_np = np.stack(res_list)\n",
    "    #res_np = res_np.mean(axis=0)\n",
    "    return res_np, target_list, pred_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if getattr(m, 'bias') is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f73338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightConstraint(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'weight'):\n",
    "            w=module.weight.data\n",
    "            w=w.clamp(0, 1)\n",
    "            module.weight.data=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a84e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, fs_weight, preds, mask):\n",
    "        return self.ce(preds, mask) + torch.sum(1 - (torch.abs(fs_weight) / 0.99 - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NnModel(pl.LightningModule):\n",
    "    def __init__(self, model, loss, enable_image_logging=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.enable_image_logging = enable_image_logging\n",
    "        #self.weight_contraint_function = WeightConstraint()\n",
    "\n",
    "    def _custom_histogram_adder(self):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name,params,self.current_epoch)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        img, mask = train_batch\n",
    "        preds = self.model(img)\n",
    "        loss = self.loss(preds, mask)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return batch\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print('Size epoch end input: ', len(outputs))\n",
    "        metric, target_list, pred_list, loss_list = calculate_iou(outputs, self.model, loss=self.loss)\n",
    "        for batch_idx, (loss_s, metric_s, target_s, pred_s) in enumerate(zip(loss_list, metric, target_list, pred_list)):\n",
    "            if self.enable_image_logging:\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                sns.heatmap(pred_s, ax=ax1, vmin=0, vmax=17)\n",
    "                sns.heatmap(target_s, ax=ax2, vmin=0, vmax=17)\n",
    "                fig.savefig('temp_fig.png')\n",
    "                plt.close(fig)\n",
    "\n",
    "    #             trainer.logger.experiment.log_histogram_3d(\n",
    "    #                 self.model.features_selection.weight.detach().cpu().numpy(),\n",
    "    #                 name='band-selection layer',\n",
    "    #                 step=self.global_step\n",
    "    #             )\n",
    "                if hasattr(trainer.logger.experiment, 'log_image'):\n",
    "                    # For Comet logger\n",
    "                    trainer.logger.experiment.log_image(\n",
    "                        'temp_fig.png', name=f'{batch_idx}', \n",
    "                        overwrite=False, step=self.global_step\n",
    "                    )\n",
    "                else:\n",
    "                    # For tensorboard logger\n",
    "                    img = cv2.imread('temp_fig.png')\n",
    "                    trainer.logger.experiment.add_image(f'{batch_idx}', img, dataformats='HWC')\n",
    "\n",
    "            d = {f'iou_{i}': iou for i, iou in enumerate(metric_s)}\n",
    "            self.log_dict(d, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            d = {f'loss_image_{batch_idx}': torch.tensor(loss_s, dtype=torch.float) }\n",
    "            self.log_dict(d, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                f\"mean_iou_class_{i}\": torch.tensor(iou, dtype=torch.float)\n",
    "                for i, iou in enumerate(metric.mean(axis=0))\n",
    "            },\n",
    "            on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"mean_iou\": torch.tensor(np.asarray(metric).mean(), dtype=torch.float),\n",
    "            },\n",
    "            on_step=False, on_epoch=True, prog_bar=True\n",
    "        )\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"mean_loss\": torch.tensor(np.asarray(loss_list).mean(), dtype=torch.float),\n",
    "            },\n",
    "            on_step=False, on_epoch=True, prog_bar=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_explained_variance = np.load('PcaExplainedVariance_.npy')\n",
    "pca_mean = np.load('PcaMean.npy')\n",
    "pca_components = np.load('PcaComponents.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transformation(x):\n",
    "    x_t = np.reshape(x, (x.shape[0], -1)) # (C, H, W) -> (C, H * W)\n",
    "    x_t = np.swapaxes(x_t, 0, 1) # (C, H * W) -> (H * W, C)\n",
    "    x_t = x_t - pca_mean\n",
    "    x_t = np.dot(x_t, pca_components.T) / np.sqrt(pca_explained_variance)\n",
    "    return np.reshape(x_t, (x.shape[1], x.shape[2], pca_components.shape[0])).astype(np.float32) # (H, W, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_old(imgs, masks):\n",
    "    target_size = (256, 256)\n",
    "    _images = [image.resize(target_size,Image.BILINEAR)\n",
    "                   for image in imgs]\n",
    "    _masks = [mask.resize(target_size, Image.BILINEAR) for mask in masks]\n",
    "    return _images, _masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55211d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mask(mask):\n",
    "    kernel = np.ones((2,2),np.uint8)\n",
    "\n",
    "    erosion = cv2.erode(mask, kernel, iterations = 2)\n",
    "    dilation = cv2.dilate(erosion, kernel,iterations = 4)\n",
    "    mask_filtered = cv2.erode(dilation, kernel, iterations = 2)\n",
    "    return mask_filtered\n",
    "\n",
    "def preprocessing(imgs, masks):\n",
    "    with open('data_standartization_params.json', 'r') as f:\n",
    "        data_standartization_params = json.load(f)\n",
    "    mean = data_standartization_params.get('means')\n",
    "    std = data_standartization_params.get('stds')\n",
    "    def standartization(img):\n",
    "        return np.array((img - mean) / std, dtype=np.float32)\n",
    "    _images = [pca_transformation(image) for image in imgs]\n",
    "    _images = [standartization(image) for image in _images]\n",
    "    _masks = [\n",
    "        np.expand_dims(\n",
    "            preprocess_mask(cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY).astype(np.uint8))\n",
    "            ,0\n",
    "        ).astype(np.int64)\n",
    "        for mask in masks\n",
    "    ]\n",
    "    return _images, _masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69079a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_augmentation(image, mask):\n",
    "    image = TF.to_tensor(image)\n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    mask = torch.from_numpy(mask)\n",
    "    \n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16432260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(image, mask):\n",
    "    image = TF.to_tensor(image)\n",
    "    mask = torch.from_numpy(mask)\n",
    "    angle = T.RandomRotation.get_params((-30, 30))\n",
    "    image = TF.rotate(image, angle, interpolation=T.InterpolationMode.BILINEAR)\n",
    "    mask = TF.rotate(mask, angle, interpolation=T.InterpolationMode.NEAREST)\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "        mask = TF.hflip(mask)\n",
    "\n",
    "    if np.random.random() > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "        mask = TF.vflip(mask)\n",
    "    \n",
    "    #image = (image - image.min()) / (image.max() - image.min())\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random = np.random.permutation(np.arange(384))\n",
    "# test_indices = random[310:]\n",
    "# train_indices = random[:310]\n",
    "\n",
    "test_indices = np.load('test_indices.npy')\n",
    "train_indices = np.load('train_indices.npy')\n",
    "path = '/raid/rustam/hyperspectral_dataset/cropped_hsi_data'\n",
    "\n",
    "dataset_train = HsiDataloader(\n",
    "    path, preprocessing=preprocessing, \n",
    "    augmentation=augmentation, indices=train_indices,\n",
    "    shuffle_data=True\n",
    ")\n",
    "dataset_test = HsiDataloader(path, preprocessing=preprocessing, augmentation=test_augmentation, indices=test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb68910",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19733c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i = np.transpose(data[0], (0, 1, 2, 3))\n",
    "data_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].cpu().detach().numpy().shape, data[1].cpu().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e51c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cv2.medianBlur(data[1][0].cpu().detach().numpy().astype(np.uint8), 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb005fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_i[0][9].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data_i[0][7].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcee1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.transpose(data[0][0].cpu().detach().numpy(), (1, 2, 0))[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.transpose(data[0][0].cpu().detach().numpy(), (1, 2, 0))[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ee19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba761c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySuperNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f=17, out_f=17):\n",
    "        super().__init__()\n",
    "        self.bn_start = nn.BatchNorm2d(in_f)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_f, in_f * 4, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(in_f * 4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_f * 4, in_f * 8, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(in_f * 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_f * 8, in_f * 4, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(in_f * 4)\n",
    "        self.act3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_f * 4, in_f, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(in_f)\n",
    "        self.act4 = nn.ReLU()\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(in_f, out_f, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.bn_start(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a17000",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MySuperNet(17, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3705d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb451589",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(torch.randn(1, 17, 512, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CometLogger(\n",
    "    api_key=\"your-key\",\n",
    "    workspace=\"your-workspace\",  # Optional\n",
    "    project_name=\"your-project-name\",  # Optional\n",
    "    experiment_name=\"new IOU//lower arch//50ep.W PCA.// RustamPreprocess(k=2) /makiloss/gamma=4/balance=2\"\n",
    ")\n",
    "\n",
    "#logger = TensorBoardLogger(\n",
    "#    'logs/'\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLossCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    copy from: https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/FocalLoss/FocalLoss.py\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
    "    :param num_class:\n",
    "    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
    "    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
    "                    focus on hard misclassified example\n",
    "    :param smooth: (float,double) smooth value when cross entropy\n",
    "    :param balance_index: (int) balance class index, should be specific when alpha is float\n",
    "    :param size_average: (bool, optional) By default, the losses are averaged over each loss element in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=None, gamma=4, balance_index=2, smooth=1e-5, size_average=False):\n",
    "        super(FocalLossCustom, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.balance_index = balance_index\n",
    "        self.smooth = smooth\n",
    "        self.size_average = size_average\n",
    "        self.cel = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        if self.smooth is not None:\n",
    "            if self.smooth < 0 or self.smooth > 1.0:\n",
    "                raise ValueError('smooth value should be in [0,1]')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        num_class = logit.shape[1]\n",
    "\n",
    "        if logit.dim() > 2:\n",
    "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            logit = logit.view(logit.size(0), logit.size(1), -1)\n",
    "            logit = logit.permute(0, 2, 1).contiguous()\n",
    "            logit = logit.view(-1, logit.size(-1))\n",
    "        target = torch.squeeze(target, 1)\n",
    "        target = target.view(-1, 1)\n",
    "        \n",
    "        ce_loss = self.cel(logit, target.view(-1))\n",
    "        train_conf = self.softmax(logit)\n",
    "        \n",
    "        idx = target.cpu().long()\n",
    "        one_hot_labels  = torch.FloatTensor(target.size(0), num_class).zero_()\n",
    "        one_hot_labels  = one_hot_labels.scatter_(1, idx, 1)\n",
    "        if one_hot_labels.device != logit.device:\n",
    "            one_hot_labels = one_hot_labels.to(logit.device)\n",
    "        \n",
    "        filtered_conf = train_conf * one_hot_labels\n",
    "        sparce_conf, _ = torch.max(filtered_conf, dim=-1)\n",
    "        loss = torch.pow((torch.ones_like(sparce_conf) - sparce_conf), self.gamma) * ce_loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(logit.shape, target.shape)\n",
    "        # \n",
    "        alpha = self.alpha\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = torch.ones(num_class, 1)\n",
    "        elif isinstance(alpha, (list, np.ndarray)):\n",
    "            assert len(alpha) == num_class\n",
    "            alpha = torch.FloatTensor(alpha).view(num_class, 1)\n",
    "            alpha = alpha / alpha.sum()\n",
    "        elif isinstance(alpha, float):\n",
    "            alpha = torch.ones(num_class, 1)\n",
    "            alpha = alpha * (1 - self.alpha)\n",
    "            alpha[self.balance_index] = self.alpha\n",
    "\n",
    "        else:\n",
    "            raise TypeError('Not support alpha type')\n",
    "        \n",
    "        if alpha.device != logit.device:\n",
    "            alpha = alpha.to(logit.device)\n",
    "\n",
    "        idx = target.cpu().long()\n",
    "\n",
    "        one_hot_key = torch.FloatTensor(target.size(0), num_class).zero_()\n",
    "        one_hot_key = one_hot_key.scatter_(1, idx, 1)\n",
    "        if one_hot_key.device != logit.device:\n",
    "            one_hot_key = one_hot_key.to(logit.device)\n",
    "\n",
    "        if self.smooth:\n",
    "            one_hot_key = torch.clamp(\n",
    "                one_hot_key, self.smooth/(num_class-1), 1.0 - self.smooth)\n",
    "        pt = (one_hot_key * logit).sum(1) + self.smooth\n",
    "        logpt = pt.log()\n",
    "        \n",
    "        gamma = self.gamma\n",
    "\n",
    "        alpha = alpha[idx]\n",
    "        alpha = torch.squeeze(alpha)\n",
    "        loss = -1 * alpha * torch.pow((1 - pt), gamma) * logpt\n",
    "        \"\"\"\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        if not self.size_average:\n",
    "            # Norm by positive\n",
    "            num_positive = torch.sum(target != self.balance_index)\n",
    "            loss = loss.sum() / (num_positive + 1e-10)\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72220c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = NnModel(net, muti_bce_loss_fusion, enable_image_logging=True)\n",
    "model = NnModel(net, FocalLossCustom(), enable_image_logging=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=50,\n",
    "    check_val_every_n_epoch=2,\n",
    "    logger=logger\n",
    ")\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=1, \n",
    "#     max_epochs=2000,\n",
    "#     check_val_every_n_epoch=2000)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.experiment.log_html(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ba81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fd613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab544292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de422ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "np.set_printoptions(suppress=True)\n",
    "def matrix2onehot(matrix, num_classes=17):\n",
    "    matrix = matrix.copy().reshape(-1)\n",
    "    one_hoted = np.zeros((matrix.size, num_classes))\n",
    "    one_hoted[np.arange(matrix.size),matrix] = 1\n",
    "    return one_hoted\n",
    "\n",
    "\n",
    "def calculate_iou(eval_loader, model, num_classes=17):\n",
    "    res_list = []\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    for in_data_x, val_data in iter(eval_loader):\n",
    "        #preds = nn.functional.softmax(model(in_data_x), dim=1).cpu().detach().numpy()\n",
    "        preds = model(in_data_x.cpu()).cpu().detach().numpy()\n",
    "        preds = np.squeeze(np.argmax(preds, axis=1))\n",
    "        target = np.squeeze(val_data.cpu().detach().numpy())\n",
    "        \n",
    "        target_list.append(target)\n",
    "        pred_list.append(preds)\n",
    "        \n",
    "        preds_one_hoted = matrix2onehot(preds, num_classes=num_classes)\n",
    "        target_one_hoted = matrix2onehot(target, num_classes=num_classes)\n",
    "        res = jaccard_score(target_one_hoted, preds_one_hoted, average=None, zero_division=0)\n",
    "        res_list.append(\n",
    "            res\n",
    "        )\n",
    "    \n",
    "    res_np = np.stack(res_list)\n",
    "    res_np = res_np.mean(axis=0)\n",
    "    return res_np, target_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, target_list, pred_list,_ = calculate_iou(val_loader, net, num_classes=17)\n",
    "res, res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753005db",
   "metadata": {},
   "outputs": [],
   "source": [
    "iiiii = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(target_list[iiiii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pred_list[iiiii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693049af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7f588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bb112",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_val = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data_x, val_data = next(iter_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff03014",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = net(in_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np, val_data_np = preds.detach().numpy(), val_data.detach().numpy()\n",
    "preds_np.shape, val_data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np = np.argmax(preds_np, axis=1)\n",
    "preds_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac988ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_numpy(np.expand_dims(preds_np, axis=0), val_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(preds_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28596c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(val_data_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(in_data_x[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ffe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
