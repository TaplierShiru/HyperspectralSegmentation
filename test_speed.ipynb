{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845272b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/makitorch')\n",
    "sys.path.append('/home/rustam/hyperspecter_segmentation/')\n",
    "\n",
    "PREFIX_INFO_PATH = '/home/rustam/hyperspecter_segmentation/danil_cave/kfolds_data/kfold0'\n",
    "PATH_DATA = '/raid/rustam/hyperspectral_dataset/new_cropped_hsi_data'\n",
    "\n",
    "\n",
    "from multiprocessing.dummy import Pool\n",
    "from multiprocessing import shared_memory\n",
    "\n",
    "from makitorch import *\n",
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import comet_ml\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "from Losses import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from makitorch.architectures.U2Net import U2Net\n",
    "\n",
    "from hsi_dataset_api import HsiDataset\n",
    "\n",
    "from makitorch.dataloaders.HsiDataloader import HsiDataloader\n",
    "from makitorch.architectures.Unet import Unet, UnetWithFeatureSelection\n",
    "from makitorch.loss import muti_bce_loss_fusion\n",
    "from sklearn.metrics import jaccard_score\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "from makitorch.data_tools.augmentation import DataAugmentator\n",
    "from makitorch.data_tools.augmentation import BaseDataAugmentor\n",
    "from makitorch.data_tools.preprocessing import BaseDataPreprocessor\n",
    "from makitorch.data_tools.preprocessing import DataPreprocessor\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from hsi_dataset_api import HsiDataset\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea56e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb08f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_into_parts_model_input(\n",
    "        image: np.ndarray, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    image_parts_list = []\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            img_part = image[:, :,  \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ]\n",
    "            image_parts_list.append(img_part)\n",
    "    return image_parts_list\n",
    "\n",
    "\n",
    "def merge_parts_into_single_mask(\n",
    "        preds, shape, h_parts: int, \n",
    "        w_parts: int, h_win: int, w_win: int):\n",
    "    pred_mask = torch.zeros(\n",
    "        shape,\n",
    "        dtype=preds.dtype, device=preds.device\n",
    "    )\n",
    "    counter = 0\n",
    "\n",
    "    for h_i in range(h_parts):\n",
    "        for w_i in range(w_parts):\n",
    "            pred_mask[:, :,  \n",
    "                h_i * h_win: (h_i+1) * h_win, \n",
    "                w_i * w_win: (w_i+1) * w_win\n",
    "            ] = preds[counter]\n",
    "            counter += 1\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def collect_prediction_and_target(eval_loader, model, cut_window=(8, 8), image_shape=(512, 512), num_classes=17):\n",
    "    target_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    for in_data_x, val_data in tqdm(iter(eval_loader)):\n",
    "        batch_size = in_data_x.shape[0]\n",
    "        # We will cut image into peases and stack it into single BIG batch\n",
    "        h_win, w_win = cut_window\n",
    "        h_parts, w_parts = image_shape[1] // w_win, image_shape[0] // h_win\n",
    "        in_data_x_parts_list = cut_into_parts_model_input(\n",
    "            in_data_x, h_parts=h_parts, \n",
    "            w_parts=w_parts, h_win=h_win, w_win=w_win\n",
    "        )\n",
    "        in_data_x_batch = torch.cat(in_data_x_parts_list, dim=0) # (N, 17, 1, 1)\n",
    "        # Make predictions\n",
    "        preds = model(in_data_x_batch) # (N, num_classes, 8, 8)\n",
    "        # Create full image again from peases\n",
    "        pred_mask = merge_parts_into_single_mask(\n",
    "            preds=preds, shape=(batch_size, num_classes, image_shape[0], image_shape[1]), \n",
    "            h_parts=h_parts, w_parts=w_parts, h_win=h_win, w_win=w_win\n",
    "        )\n",
    "        target_list.append(val_data)\n",
    "        pred_list.append(pred_mask)\n",
    "    return (torch.cat(pred_list, dim=0), \n",
    "            torch.cat(target_list, dim=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def matrix2onehot(matrix, num_classes=17):\n",
    "    matrix = matrix.copy().reshape(-1)\n",
    "    one_hoted = np.zeros((matrix.size, num_classes))\n",
    "    one_hoted[np.arange(matrix.size),matrix] = 1\n",
    "    return one_hoted\n",
    "\n",
    "def list_target_to_onehot(target_tensor, num_classes=17):\n",
    "    one_hoted_list = []\n",
    "    for target in target_tensor:\n",
    "        # target - (H, W)\n",
    "        target =  target.cpu().detach().numpy()\n",
    "        h,w = target.shape\n",
    "        target = matrix2onehot(target, num_classes=num_classes)\n",
    "        target = target.reshape(h, w, -1)\n",
    "        target = np.transpose(target, [2, 0, 1])\n",
    "        one_hoted_list.append(target)\n",
    "    return torch.from_numpy(np.stack(one_hoted_list, axis=0))\n",
    "        \n",
    "\n",
    "def calculate_iou(pred_list, target_list, num_classes=17):\n",
    "    res_list = []\n",
    "    pred_as_mask_list = []\n",
    "    \n",
    "    for preds, target in zip(pred_list, target_list):\n",
    "        # preds - (num_classes, H, W)\n",
    "        preds = preds.detach()\n",
    "        # target - (H, W)\n",
    "        target = target.detach()\n",
    "\n",
    "        preds = nn.functional.softmax(preds, dim=0)\n",
    "        preds = torch.argmax(preds, dim=0)\n",
    "        pred_as_mask_list.append(preds)\n",
    "        \n",
    "        preds_one_hoted = torch.nn.functional.one_hot(preds, num_classes).view(-1, num_classes)\n",
    "        target_one_hoted = torch.nn.functional.one_hot(target, num_classes).view(-1, num_classes)\n",
    "        res = jaccard_score(target_one_hoted, preds_one_hoted, average=None, zero_division=1)\n",
    "        res_list.append(\n",
    "            res\n",
    "        )\n",
    "    \n",
    "    res_np = np.stack(res_list)\n",
    "    #res_np = res_np.mean(axis=0)\n",
    "    return res_np, pred_as_mask_list\n",
    "\n",
    "\n",
    "def dice_loss(preds, ground_truth, eps=1e-5, dim=None, use_softmax=False, softmax_dim=1):\n",
    "    \"\"\"\n",
    "    Computes Dice loss according to the formula from:\n",
    "    V-Net: Fully Convolutional Neural Networks forVolumetric Medical Image Segmentation\n",
    "    Link to the paper: http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : tf.Tensor\n",
    "        Predicted probabilities.\n",
    "    ground_truth : tf.Tensor\n",
    "        Ground truth labels.\n",
    "    eps : float\n",
    "        Used to prevent division by zero in the Dice denominator.\n",
    "    axes : list\n",
    "        Defines which axes the dice value will be computed on. The computed dice values will be averaged\n",
    "        along the remaining axes. If None, Dice is computed on an entire batch.\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Scalar dice loss tensor.\n",
    "    \"\"\"\n",
    "    ground_truth = ground_truth.float().to(device=preds.device)\n",
    "    \n",
    "    if use_softmax:\n",
    "        preds = nn.functional.softmax(preds, dim=softmax_dim)\n",
    "    \n",
    "    numerator = preds * ground_truth\n",
    "    numerator = torch.sum(numerator, dim=dim)\n",
    "\n",
    "    p_squared = torch.square(preds)\n",
    "    p_squared = torch.sum(p_squared, dim=dim)\n",
    "    # ground_truth is not squared to avoid unnecessary computation.\n",
    "    # 0^2 = 0\n",
    "    # 1^2 = 1\n",
    "    g_squared = torch.sum(torch.square(ground_truth), dim=dim)\n",
    "    denominator = p_squared + g_squared + eps\n",
    "\n",
    "    dice = 2 * numerator / denominator\n",
    "    return 1 - dice\n",
    "\n",
    "def clear_metric_calculation(final_metric, target_t, pred_t, num_classes=17):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    final_metric: torch.Tensor\n",
    "        Tensor with shape (N, C)\n",
    "    target_t: torch.Tensor or list\n",
    "        Tensor with shape (N, 1, H, W)\n",
    "    pred_t: torch.Tensor or list\n",
    "        Tensor with shape (N, 1, H, W)\n",
    "    \n",
    "    \"\"\"\n",
    "    # For each image\n",
    "    final_metric_dict = dict([\n",
    "        (str(i), []) for i in range(num_classes)\n",
    "    ])\n",
    "    for metric_s, target_t_s, pred_t_s in zip(final_metric, target_t, pred_t):\n",
    "        unique_indx_target = torch.unique(target_t_s.long())\n",
    "        if isinstance(pred_t_s, np.ndarray):\n",
    "            pred_t_s = torch.from_numpy(pred_t_s)\n",
    "        unique_indx_pred = torch.unique(pred_t_s.long())\n",
    "        for i in range(num_classes):\n",
    "            if i in unique_indx_target or i in unique_indx_pred:\n",
    "                final_metric_dict[str(i)].append(metric_s[i])\n",
    "    \n",
    "    mean_per_class_metric = [\n",
    "        sum(final_metric_dict[str(i)]) / len(final_metric_dict[str(i)])\n",
    "        if len(final_metric_dict[str(i)]) != 0\n",
    "        else 0.0\n",
    "        for i in range(num_classes)\n",
    "    ] \n",
    "    mean_metric = sum(mean_per_class_metric) / len(mean_per_class_metric)\n",
    "    return mean_per_class_metric, mean_metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MySuperNetLittleInput(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_f=237, out_f=17, *args):\n",
    "        super().__init__()\n",
    "        #self.bn_start = nn.BatchNorm3d(in_f)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_f, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # (N, 128, 8, 8)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # (N, 128, 8, 8)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # (N, 64, 8, 8)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.act3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # (N, 64, 8, 8)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, out_f, kernel_size=3, stride=1, padding=1)\n",
    "        # (N, 17, 8, 8)\n",
    "        self.bn5 = nn.BatchNorm2d(out_f)\n",
    "        self.act5 = nn.ReLU()\n",
    "\n",
    "        self.final_conv = nn.Conv2d(out_f, out_f, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.act5(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069af2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = MySuperNetLittleInput(in_f=17, out_f=17)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    _ = net(torch.randn(1, 17, 128, 128))\n",
    "#net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_input = torch.rand(39, 17, 512, 512, dtype=torch.float)\n",
    "fake_masks = torch.randint(0, 17, size=(39, 512, 512)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e502e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(fake_input, fake_masks):\n",
    "    for f_i, f_m in zip(fake_input, fake_masks):\n",
    "        yield torch.unsqueeze(f_i, dim=0), torch.unsqueeze(f_m, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_g = data_generator(fake_input, fake_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tensor, target_tensor = collect_prediction_and_target(data_g, net, cut_window=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "target_one_hotted_tensor = list_target_to_onehot(target_tensor)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_one_hotted_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff393c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "target_one_hotted_tensor_n = torch.nn.functional.one_hot(\n",
    "    target_tensor, 17 # Num classes\n",
    ")\n",
    "target_one_hotted_tensor_n = target_one_hotted_tensor_n.permute(0, -1, 1, 2)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_one_hotted_tensor_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((target_one_hotted_tensor_n == target_one_hotted_tensor).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7bbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dice_loss_val = dice_loss(pred_tensor, target_one_hotted_tensor, dim=[0, 2, 3], use_softmax=True, softmax_dim=1)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08474d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "metric, pred_as_mask_list = calculate_iou(pred_tensor, target_tensor)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "metric, pred_as_mask_list = calculate_iou(pred_tensor, target_tensor)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_as_mask_list[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6ae36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e324e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(pred_as_mask_list[0], np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d9c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clear_metric_calculation(metric, target_tensor, pred_as_mask_list)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ed64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clear_metric_calculation(metric, target_tensor, pred_as_mask_list)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_np = pred_tensor[0].detach().numpy().copy()\n",
    "d_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a008273",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "_ = np.unique(d_np)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "_ = torch.unique(pred_tensor[0])\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500ac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_tensor[0].cpu().detach().numpy()\n",
    "target_one_hoted = matrix2onehot(target, num_classes=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5217481",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_one_hoted_t = torch.nn.functional.one_hot(target_tensor[0], 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75939c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_one_hoted[1000:1010].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_one_hoted_t.view(-1, 17)[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ee9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(target_one_hoted.astype(np.float32) == target_one_hoted_t.view(-1, 17).numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e24df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(target_tensor[0], np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ac35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b46fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sns.heatmap(target_tensor[0])\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f29803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
